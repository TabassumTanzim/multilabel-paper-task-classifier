{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cc0b74eadf4c410db0de91e0049057a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c96aa07be0945eb8d874331ff37bee9","IPY_MODEL_2d0c817409e2431390c0c26e09fad407","IPY_MODEL_059a8d2fb7a24e17a16cefc3cea18bd2"],"layout":"IPY_MODEL_c9ad27d63ecb4069994b2aba900de443"}},"1c96aa07be0945eb8d874331ff37bee9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbc8bcaa281f49a4be0bba5bccfa022f","placeholder":"​","style":"IPY_MODEL_568eda7c147e451d812bb1c766fc7786","value":"Downloading builder script: "}},"2d0c817409e2431390c0c26e09fad407":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4580eba16ce74d86a132062ff5b29bf2","max":1715,"min":0,"orientation":"horizontal","style":"IPY_MODEL_006141c133f74ec3b02ca0ab65b1249f","value":1715}},"059a8d2fb7a24e17a16cefc3cea18bd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d39908ff48450ab8349d8eb87766fe","placeholder":"​","style":"IPY_MODEL_7ce78bb78d2741198e4c0b5f94fd5099","value":" 4.50k/? [00:00&lt;00:00, 89.3kB/s]"}},"c9ad27d63ecb4069994b2aba900de443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbc8bcaa281f49a4be0bba5bccfa022f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"568eda7c147e451d812bb1c766fc7786":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4580eba16ce74d86a132062ff5b29bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"006141c133f74ec3b02ca0ab65b1249f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86d39908ff48450ab8349d8eb87766fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce78bb78d2741198e4c0b5f94fd5099":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d283a4cabd4a4d8e922c6f6e2be73884":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74ea9e2ee35f4a6889024e5dea57cfb2","IPY_MODEL_94bc27b98b534314898d30a05aa1008a","IPY_MODEL_ce1646f77cf146ffaed027025d4d67a9"],"layout":"IPY_MODEL_9d83f078659c41a4b88dc8a3a3d8e75a"}},"74ea9e2ee35f4a6889024e5dea57cfb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44dd110f5dc542748f239469d2d5deb0","placeholder":"​","style":"IPY_MODEL_e2590d125c1a4fbfa4e8b270ba6f3ced","value":"Downloading extra modules: "}},"94bc27b98b534314898d30a05aa1008a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_81658306b63e40cc87a26398db84c822","max":1109,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c559888183254b1497a6cad2b18ce3fa","value":1109}},"ce1646f77cf146ffaed027025d4d67a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_024588bbbcdd4d04b4c42a296a95ab47","placeholder":"​","style":"IPY_MODEL_783d9e6d6c0240c6ba649b0a3e2f523c","value":" 3.30k/? [00:00&lt;00:00, 75.7kB/s]"}},"9d83f078659c41a4b88dc8a3a3d8e75a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44dd110f5dc542748f239469d2d5deb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2590d125c1a4fbfa4e8b270ba6f3ced":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81658306b63e40cc87a26398db84c822":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c559888183254b1497a6cad2b18ce3fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"024588bbbcdd4d04b4c42a296a95ab47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"783d9e6d6c0240c6ba649b0a3e2f523c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb19217cbdd34f4eb8fd13bc3b751952":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c093c54b6424b1eb0486dfbdc666e84","IPY_MODEL_8139c698110b44ad9cc99f2ec96c51ac","IPY_MODEL_2bd5977709f8402b81787c05c8a0ec3e"],"layout":"IPY_MODEL_2d14476e313d41879aa7f2375fef9cc6"}},"9c093c54b6424b1eb0486dfbdc666e84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02fc1d9db9034e30bef1363f45fa068b","placeholder":"​","style":"IPY_MODEL_f40fe8f29f7f435dbf0c4b55fadc6592","value":"100%"}},"8139c698110b44ad9cc99f2ec96c51ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4ee1656dfb546be8c87d37fece92048","max":2662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d50b488c260642c4b3d116aaa4c893d3","value":2662}},"2bd5977709f8402b81787c05c8a0ec3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba5914ec4f394f40bd0151a6daeaba9f","placeholder":"​","style":"IPY_MODEL_66f5badcc4a04bef9ce6be971bfa2705","value":" 2662/2662 [14:18&lt;00:00,  3.44it/s]"}},"2d14476e313d41879aa7f2375fef9cc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02fc1d9db9034e30bef1363f45fa068b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f40fe8f29f7f435dbf0c4b55fadc6592":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4ee1656dfb546be8c87d37fece92048":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d50b488c260642c4b3d116aaa4c893d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba5914ec4f394f40bd0151a6daeaba9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66f5badcc4a04bef9ce6be971bfa2705":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6946dbd9b51740e78f39f4ff4433a404":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_000a6a0d527145f5b0964b0c758aa3a7","IPY_MODEL_640cb6f2bb5845f9a423a310a651a4c7","IPY_MODEL_aa53d63d59ed48a0baad3879e620ee7d"],"layout":"IPY_MODEL_e4cf573fda9748a7a044635165e5eb07"}},"000a6a0d527145f5b0964b0c758aa3a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_491566a6c56a4e09b8a8e46b754b2b4d","placeholder":"​","style":"IPY_MODEL_312209e0efd4479d8b39163b4d98f98c","value":"config.json: 100%"}},"640cb6f2bb5845f9a423a310a651a4c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af7ad166edc24570b74ef5b21fd872e8","max":480,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4f5ba58fa0540acacc72951609090d1","value":480}},"aa53d63d59ed48a0baad3879e620ee7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_629d433ef3db45cf9cad264cc45b9445","placeholder":"​","style":"IPY_MODEL_b84725905ca6402e8172f65a4d253a59","value":" 480/480 [00:00&lt;00:00, 34.7kB/s]"}},"e4cf573fda9748a7a044635165e5eb07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"491566a6c56a4e09b8a8e46b754b2b4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"312209e0efd4479d8b39163b4d98f98c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af7ad166edc24570b74ef5b21fd872e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4f5ba58fa0540acacc72951609090d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"629d433ef3db45cf9cad264cc45b9445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b84725905ca6402e8172f65a4d253a59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7117d129c64e482ba2a9f7d4ad3c47f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20d023a56e9841cc8bccdef44c25137a","IPY_MODEL_336e2dc6d1074f30aa60bee1e71f5dde","IPY_MODEL_d0e2c785bfe64b41a0264a40b6a4a235"],"layout":"IPY_MODEL_8e267ce2f0c0495eb2c860b658a55489"}},"20d023a56e9841cc8bccdef44c25137a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb9ad6e37fbd4ff3b1ce0bfa574a5b91","placeholder":"​","style":"IPY_MODEL_d63e0678aa7446659e372ffacfac939f","value":"vocab.json: 100%"}},"336e2dc6d1074f30aa60bee1e71f5dde":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1384c9243b64d97b1160de2959bedc4","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58267cb694e0449c93aef33395d1064e","value":898823}},"d0e2c785bfe64b41a0264a40b6a4a235":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3e3767727194bc0ac6323818a7a1ef1","placeholder":"​","style":"IPY_MODEL_b643eba3824d457abe321be552ffdd97","value":" 899k/899k [00:00&lt;00:00, 1.26MB/s]"}},"8e267ce2f0c0495eb2c860b658a55489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9ad6e37fbd4ff3b1ce0bfa574a5b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d63e0678aa7446659e372ffacfac939f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1384c9243b64d97b1160de2959bedc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58267cb694e0449c93aef33395d1064e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3e3767727194bc0ac6323818a7a1ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b643eba3824d457abe321be552ffdd97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3438abe56be46a98990783c2cab570d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9c32d832548423baacc9f440c07854e","IPY_MODEL_67124991950f4e58a671cd933fe1fe4f","IPY_MODEL_f7bf5ee1e38c48c69f708ac0fd45dfe8"],"layout":"IPY_MODEL_e6e44de7bac642f292d9974e1df6a600"}},"c9c32d832548423baacc9f440c07854e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0004d129f70c465898028a42d52f6587","placeholder":"​","style":"IPY_MODEL_a500df2904f940e4b01b48ec6b2f919b","value":"merges.txt: 100%"}},"67124991950f4e58a671cd933fe1fe4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9128edbd44c94aefa1f134f4c0e4a390","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29a5103dc5f64c479be2225e21d842f9","value":456318}},"f7bf5ee1e38c48c69f708ac0fd45dfe8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ce3ff57d70142d0be875e5c4369bd86","placeholder":"​","style":"IPY_MODEL_6189fbf5150243b881f8c4d7377f8d5c","value":" 456k/456k [00:00&lt;00:00, 642kB/s]"}},"e6e44de7bac642f292d9974e1df6a600":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0004d129f70c465898028a42d52f6587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a500df2904f940e4b01b48ec6b2f919b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9128edbd44c94aefa1f134f4c0e4a390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a5103dc5f64c479be2225e21d842f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ce3ff57d70142d0be875e5c4369bd86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6189fbf5150243b881f8c4d7377f8d5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bd7909a3ce6411a82cdb661db9accf9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed5582648e034e37924da2fea3823146","IPY_MODEL_fb5dd050abe7499eaaeec916de66412c","IPY_MODEL_e287f2ecb9d34d1895b0c10136b7882b"],"layout":"IPY_MODEL_2ea4d0f72f734c4b97ff50eb6460cc29"}},"ed5582648e034e37924da2fea3823146":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6f2abb4c7a4393aba8a50d5336ed0a","placeholder":"​","style":"IPY_MODEL_b61ea826be784fe6969771dfb15116be","value":"tokenizer.json: 100%"}},"fb5dd050abe7499eaaeec916de66412c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88d564c8c9bf4bb1907941cdbc451ba5","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1cd3ded7eafe4b29be0d2f04f8d6e133","value":1355863}},"e287f2ecb9d34d1895b0c10136b7882b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cba8b8a18246477494cc37fa166cb029","placeholder":"​","style":"IPY_MODEL_a869b9c652dd4f729dcf0cfe1c8d7de4","value":" 1.36M/1.36M [00:00&lt;00:00, 1.39MB/s]"}},"2ea4d0f72f734c4b97ff50eb6460cc29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d6f2abb4c7a4393aba8a50d5336ed0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b61ea826be784fe6969771dfb15116be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88d564c8c9bf4bb1907941cdbc451ba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cd3ded7eafe4b29be0d2f04f8d6e133":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cba8b8a18246477494cc37fa166cb029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a869b9c652dd4f729dcf0cfe1c8d7de4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d9054a0bcd64b969a737263ed167a92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a127127d3ddf4c8cb069f9d8eb4e2e7b","IPY_MODEL_0452d6fee141448cb475235ec4e52057","IPY_MODEL_5d8ccfe08fd642f9b65e2014bd80b451"],"layout":"IPY_MODEL_a26c810d672a41f480ad7bc4e93be231"}},"a127127d3ddf4c8cb069f9d8eb4e2e7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ed5924c54f44e67a74c39f33ca37ff0","placeholder":"​","style":"IPY_MODEL_e7932a10e9b347998efe09f0c4487180","value":"100%"}},"0452d6fee141448cb475235ec4e52057":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a17509b7c5f446888041bb0ee9052c5","max":2662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41d684d543ba4736a4b21892c2c53f86","value":2662}},"5d8ccfe08fd642f9b65e2014bd80b451":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9223b10ebd8b42d4b372530ebeaea70c","placeholder":"​","style":"IPY_MODEL_425beda55be544b9a06b0da219edf75d","value":" 2662/2662 [13:17&lt;00:00,  3.75it/s]"}},"a26c810d672a41f480ad7bc4e93be231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ed5924c54f44e67a74c39f33ca37ff0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7932a10e9b347998efe09f0c4487180":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a17509b7c5f446888041bb0ee9052c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41d684d543ba4736a4b21892c2c53f86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9223b10ebd8b42d4b372530ebeaea70c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"425beda55be544b9a06b0da219edf75d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c621e95c81ae4d18823669362e5d09a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b35445ed12724b04820b1303277fcab1","IPY_MODEL_ac360da750bc422388c4258040d0031e","IPY_MODEL_b03e3f9c2a5c4412bc5380aa81139fec"],"layout":"IPY_MODEL_cd9e343a3a2c451c85139c9aa7e64c34"}},"b35445ed12724b04820b1303277fcab1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fc992abf37d4bd0ad585b4545610034","placeholder":"​","style":"IPY_MODEL_17d1843bdbe948d29a19f7788d6ac174","value":"100%"}},"ac360da750bc422388c4258040d0031e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8faba08d039045dd9cc851ea53217263","max":2662,"min":0,"orientation":"horizontal","style":"IPY_MODEL_178919022a3142f1b65d9627e5d871a6","value":2662}},"b03e3f9c2a5c4412bc5380aa81139fec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8da095032c674e7088db1ec881026f0c","placeholder":"​","style":"IPY_MODEL_9439974063584dc782a2069c9d7b9548","value":" 2662/2662 [12:19&lt;00:00,  3.72it/s]"}},"cd9e343a3a2c451c85139c9aa7e64c34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fc992abf37d4bd0ad585b4545610034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17d1843bdbe948d29a19f7788d6ac174":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8faba08d039045dd9cc851ea53217263":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178919022a3142f1b65d9627e5d871a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8da095032c674e7088db1ec881026f0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9439974063584dc782a2069c9d7b9548":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setting Up"],"metadata":{"id":"WWZfX9RGxDWn"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"qqbGE5VDwwk4","executionInfo":{"status":"ok","timestamp":1706449321726,"user_tz":-360,"elapsed":5,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"outputs":[],"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"]},{"cell_type":"code","source":["! pip install -q transformers[sentencepiece] fastai ohmeow-blurr nbdev"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LIYeFLnxHHE","executionInfo":{"status":"ok","timestamp":1706449496834,"user_tz":-360,"elapsed":156059,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"9d64ad9f-50c9-40b8-a4d8-73a030fcf56e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.1/66.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m966.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.7/58.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install torch==2.1.0 torchvision==0.10.1 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu121/torch_stable.html\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJVsLbGXQJtW","executionInfo":{"status":"ok","timestamp":1706449631617,"user_tz":-360,"elapsed":124715,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"8a38cdfa-c762-4cc9-be73-7877cc33281c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/cu121/torch_stable.html\n","Collecting torch==2.1.0\n","  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m740.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.10.1 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.0+cu121, 0.16.1, 0.16.1+cu121, 0.16.2, 0.16.2+cu121)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.10.1\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["! pip install -q onnxruntime onnx==1.10.0 onnxruntime-gpu onnxruntime_tools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J33mujA1xQ5-","executionInfo":{"status":"ok","timestamp":1706449662310,"user_tz":-360,"elapsed":14032,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"7587ba64-9e60-4ab5-8fa5-ee5098611d6f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForSequenceClassification, AutoConfig\n","from fastai.text.all import *\n","from blurr.text.data.all import *\n","from blurr.text.modeling.all import *"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203,"referenced_widgets":["cc0b74eadf4c410db0de91e0049057a1","1c96aa07be0945eb8d874331ff37bee9","2d0c817409e2431390c0c26e09fad407","059a8d2fb7a24e17a16cefc3cea18bd2","c9ad27d63ecb4069994b2aba900de443","bbc8bcaa281f49a4be0bba5bccfa022f","568eda7c147e451d812bb1c766fc7786","4580eba16ce74d86a132062ff5b29bf2","006141c133f74ec3b02ca0ab65b1249f","86d39908ff48450ab8349d8eb87766fe","7ce78bb78d2741198e4c0b5f94fd5099","d283a4cabd4a4d8e922c6f6e2be73884","74ea9e2ee35f4a6889024e5dea57cfb2","94bc27b98b534314898d30a05aa1008a","ce1646f77cf146ffaed027025d4d67a9","9d83f078659c41a4b88dc8a3a3d8e75a","44dd110f5dc542748f239469d2d5deb0","e2590d125c1a4fbfa4e8b270ba6f3ced","81658306b63e40cc87a26398db84c822","c559888183254b1497a6cad2b18ce3fa","024588bbbcdd4d04b4c42a296a95ab47","783d9e6d6c0240c6ba649b0a3e2f523c"]},"id":"MSDGY3C1xYb3","executionInfo":{"status":"ok","timestamp":1706449695145,"user_tz":-360,"elapsed":24647,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"b76f2c0d-8377-43a2-fffe-790db30001f0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/blurr/text/modeling/question_answering.py:31: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  squad_metric = load_metric(\"squad\")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad/squad.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc0b74eadf4c410db0de91e0049057a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d283a4cabd4a4d8e922c6f6e2be73884"}},"metadata":{}}]},{"cell_type":"code","source":["from tqdm.notebook import tqdm\n","import numpy as np"],"metadata":{"id":"0kamDZGsxb_f","executionInfo":{"status":"ok","timestamp":1706449700001,"user_tz":-360,"elapsed":708,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mmdjs_Vyxfye","executionInfo":{"status":"ok","timestamp":1706449739376,"user_tz":-360,"elapsed":32627,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"e435640d-b168-4687-b6ee-5b57c5ec26a9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP_Project2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8eTnIaJ4xg2j","executionInfo":{"status":"ok","timestamp":1706449756113,"user_tz":-360,"elapsed":6,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"c4b21491-918e-42bf-92ba-8db1d2f3d0fe"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP_Project2\n"]}]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"yZCgEpADxukk"}},{"cell_type":"code","source":["df = pd.read_csv(\"all_data.csv\")\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"id":"_6olMHPyxveX","executionInfo":{"status":"ok","timestamp":1706449816019,"user_tz":-360,"elapsed":1470,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"af4acd15-3a21-402c-f627-d9f78b727efb"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                    title  \\\n","0  Active Learning for Convolutional Neural Networks: A Core-Set Approach   \n","1                                 Variational Adversarial Active Learning   \n","2                                       Learning Loss for Active Learning   \n","3     Active learning in annotating micro-blogs dealing with e-reputation   \n","4                            libact: Pool-based Active Learning in Python   \n","\n","                                                                            url  \\\n","0     https://paperswithcode.com/paper/active-learning-for-convolutional-neural   \n","1      https://paperswithcode.com/paper/variational-adversarial-active-learning   \n","2                                    https://paperswithcode.com/paper/190503677   \n","3    https://paperswithcode.com/paper/active-learning-in-annotating-micro-blogs   \n","4  https://paperswithcode.com/paper/libact-pool-based-active-learning-in-python   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  abstract  \\\n","0  Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to ...   \n","1  Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled a...   \n","2  The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We...   \n","3  Elections unleash strong political views on Twitter, but what do people really think about politics? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning (ML). Since the performance of ML and Natural Language Processing (NLP) approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating...   \n","4                                                                libact is a Python package designed to make active learning easier for general users. The package not only implements several popular active learning strategies, but also features the active-learning-by-learning meta-algorithm that assists the users to automatically select the best strategy on the fly. Furthermore, the package provides a unified interface for implementing more strategies, models and application-specific labelers. The package is open-source on Github, and can be easily installed from Python Package Index repository.   \n","\n","                                                                                                    tasks  \n","0                                                             ['Active Learning', 'Image Classification']  \n","1                                    ['Active Learning', 'Image Classification', 'Semantic Segmentation']  \n","2  ['Active Learning', 'Image Classification', 'object-detection', 'Object Detection', 'Pose Estimation']  \n","3     ['Active Learning', 'Information Retrieval', 'Opinion Mining', 'Retrieval', 'Topic Classification']  \n","4                                                                                     ['Active Learning']  "],"text/html":["\n","  <div id=\"df-7281d478-e923-4a21-975c-533ee1416317\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>abstract</th>\n","      <th>tasks</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Active Learning for Convolutional Neural Networks: A Core-Set Approach</td>\n","      <td>https://paperswithcode.com/paper/active-learning-for-convolutional-neural</td>\n","      <td>Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to ...</td>\n","      <td>['Active Learning', 'Image Classification']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Variational Adversarial Active Learning</td>\n","      <td>https://paperswithcode.com/paper/variational-adversarial-active-learning</td>\n","      <td>Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled a...</td>\n","      <td>['Active Learning', 'Image Classification', 'Semantic Segmentation']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Learning Loss for Active Learning</td>\n","      <td>https://paperswithcode.com/paper/190503677</td>\n","      <td>The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We...</td>\n","      <td>['Active Learning', 'Image Classification', 'object-detection', 'Object Detection', 'Pose Estimation']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Active learning in annotating micro-blogs dealing with e-reputation</td>\n","      <td>https://paperswithcode.com/paper/active-learning-in-annotating-micro-blogs</td>\n","      <td>Elections unleash strong political views on Twitter, but what do people really think about politics? Opinion and trend mining on micro blogs dealing with politics has recently attracted researchers in several fields including Information Retrieval and Machine Learning (ML). Since the performance of ML and Natural Language Processing (NLP) approaches are limited by the amount and quality of data available, one promising alternative for some tasks is the automatic propagation of expert annotations. This paper intends to develop a so-called active learning process for automatically annotating...</td>\n","      <td>['Active Learning', 'Information Retrieval', 'Opinion Mining', 'Retrieval', 'Topic Classification']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>libact: Pool-based Active Learning in Python</td>\n","      <td>https://paperswithcode.com/paper/libact-pool-based-active-learning-in-python</td>\n","      <td>libact is a Python package designed to make active learning easier for general users. The package not only implements several popular active learning strategies, but also features the active-learning-by-learning meta-algorithm that assists the users to automatically select the best strategy on the fly. Furthermore, the package provides a unified interface for implementing more strategies, models and application-specific labelers. The package is open-source on Github, and can be easily installed from Python Package Index repository.</td>\n","      <td>['Active Learning']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7281d478-e923-4a21-975c-533ee1416317')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7281d478-e923-4a21-975c-533ee1416317 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7281d478-e923-4a21-975c-533ee1416317');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-baa7353c-810b-42f7-b5ae-03523c457610\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-baa7353c-810b-42f7-b5ae-03523c457610')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-baa7353c-810b-42f7-b5ae-03523c457610 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["df = df.dropna().reset_index(drop=True)\n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2N3pvQZyWVO","executionInfo":{"status":"ok","timestamp":1706449819879,"user_tz":-360,"elapsed":887,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"eb2fc713-3c32-40cf-f3f6-29714fd4d168"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26778, 4)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["tasks_list = df.tasks.to_list()\n","task_count = {}\n","for tasks in tasks_list:\n","  task_list = eval(tasks)\n","  for task in task_list:\n","    if task in task_count.keys():\n","      task_count[task] += 1\n","    else:\n","      task_count[task] = 1\n","print(f\"Number of Tasks: {len(task_count)}\")\n","print(task_count)\n","\n","threshold = int(len(df) * 0.002)\n","rare_tasks = [key for key, value in task_count.items() if value < threshold]\n","len(rare_tasks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sbcW-fUlyK02","executionInfo":{"status":"ok","timestamp":1706449822398,"user_tz":-360,"elapsed":7,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"18452b39-a145-484b-8f88-f21c277c7c8f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Tasks: 2397\n","{'Active Learning': 221, 'Image Classification': 897, 'Semantic Segmentation': 1932, 'object-detection': 851, 'Object Detection': 958, 'Pose Estimation': 155, 'Information Retrieval': 551, 'Opinion Mining': 53, 'Retrieval': 1536, 'Topic Classification': 35, 'Model Optimization': 16, 'valid': 140, 'Self-Supervised Learning': 403, 'Test': 804, 'Spam detection': 9, 'Benchmarking': 264, 'Segmentation': 1649, 'Clustering': 397, 'Community Detection': 65, 'Experimental Design': 17, 'Informativeness': 54, 'Federated Learning': 156, 'Gaussian Processes': 31, 'Physical Simulations': 2, 'Uncertainty Quantification': 58, 'Data Integration': 9, 'Classification': 1018, 'Domain Adaptation': 810, 'regression': 211, 'Text-to-Image Generation': 115, 'Transfer Learning': 858, 'Adversarial Attack': 84, 'Model extraction': 9, 'Decision Making': 339, 'Property Prediction': 39, 'Variational Inference': 98, '3D Object Detection': 168, 'Object': 523, 'Out-of-Distribution Detection': 48, 'Decision Making Under Uncertainty': 3, 'Depth Aleatoric Uncertainty Estimation': 3, 'Explainable Artificial Intelligence (XAI)': 24, 'Android Malware Detection': 2, 'Malware Detection': 13, 'Object Localization': 46, 'Semi-Supervised Object Detection': 14, 'Weakly Supervised Object Detection': 9, 'Drug Discovery': 53, 'speech-recognition': 260, 'Speech Recognition': 280, 'Food recommendation': 3, 'Point Cloud Segmentation': 20, 'counterfactual': 109, 'Counterfactual Reasoning': 12, 'Reinforcement Learning (RL)': 797, 'Symbolic Regression': 3, 'Graph Learning': 151, 'Node Classification': 356, 'Semi-supervised Domain Adaptation': 17, 'Source-Free Domain Adaptation': 25, 'Unsupervised Domain Adaptation': 261, 'Superpixels': 20, 'whole slide images': 22, 'Outlier Detection': 24, 'Facial Expression Recognition': 34, 'Facial Expression Recognition (FER)': 38, 'Model Selection': 69, 'Image Segmentation': 969, 'Medical Image Classification': 249, 'Medical Image Segmentation': 584, 'Continual Learning': 473, 'Data Valuation': 3, 'Brain Tumor Segmentation': 141, 'Multi-Label Classification': 108, 'Tumor Segmentation': 266, 'Bayesian Optimization': 22, 'Binary Classification': 104, 'Multimodal Deep Learning': 24, 'Anatomy': 73, 'Math': 47, 'Math Word Problem Solving': 14, 'Bayesian Inference': 48, 'Structured Prediction': 38, 'Explanation Generation': 21, 'Interactive Segmentation': 26, 'Autonomous Vehicles': 105, 'Organ Segmentation': 43, 'Surface Reconstruction': 14, 'Conversational Search': 16, 'Implicit Discourse Relation Classification': 2, 'Domain Generalization': 186, 'Anomaly Detection': 128, '3D Semantic Segmentation': 44, 'Multiobjective Optimization': 2, 'Image Generation': 953, 'Automatic Speech Recognition': 134, 'Automatic Speech Recognition (ASR)': 117, 'Dimensionality Reduction': 90, 'Change Detection': 26, 'Atari Games': 44, 'Class Incremental Learning': 53, 'General Classification': 1060, 'Incremental Learning': 91, 'reinforcement-learning': 559, 'Disjoint 10-1': 1, 'Disjoint 15-1': 1, 'Disjoint 15-5': 1, 'Domain 1-1': 1, 'Domain 11-1': 1, 'Domain 11-5': 1, 'Overlapped 10-1': 2, 'Overlapped 15-1': 2, 'Overlapped 15-5': 2, 'Permuted-MNIST': 5, 'Meta-Learning': 692, 'Object Recognition': 97, 'Hippocampus': 12, 'Network Pruning': 16, 'Knowledge Distillation': 307, 'Dataset Condensation': 6, 'Neural Architecture Search': 82, 'Fairness': 173, 'L2 Regularization': 4, 'Audio Classification': 23, 'Open Set Learning': 22, '3D Shape Reconstruction': 7, 'Representation Learning': 1483, 'Metric Learning': 112, 'Out of Distribution (OOD) Detection': 23, 'Hard Attention': 17, 'Age And Gender Classification': 4, 'Face Recognition': 617, 'Face Verification': 84, 'Gender Prediction': 4, 'Model Compression': 38, 'One-shot model fusion': 1, 'Stochastic Optimization': 15, 'Grounded language learning': 10, 'Language Acquisition': 18, 'Language Modelling': 1809, 'Explainable artificial intelligence': 19, 'Interpretable Machine Learning': 15, 'Data Augmentation': 1048, 'Action Detection': 71, 'Activity Detection': 10, 'Autonomous Driving': 559, 'Event Detection': 37, 'Self-Knowledge Distillation': 6, 'Learning Theory': 15, 'Contrastive Learning': 686, 'Face Swapping': 50, 'Fact Checking': 95, 'Question Answering': 1654, 'World Knowledge': 79, 'Natural Questions': 37, 'Sentiment Analysis': 881, 'Sentiment Classification': 176, 'Keyword Spotting': 17, 'Attribute': 379, 'Fine-Grained Image Classification': 22, 'Continuous Control': 50, 'Multi-Task Learning': 318, 'Privacy Preserving': 84, 'Model-based Reinforcement Learning': 17, 'General Knowledge': 40, 'Continual Pretraining': 3, 'One-Shot Learning': 23, 'Philosophy': 20, 'General Reinforcement Learning': 10, 'Quantization': 144, 'Rolling Shutter Correction': 4, 'Sequential Bayesian Inference': 3, 'Few-Shot Semantic Segmentation': 11, 'Video Object Segmentation': 34, 'Video Semantic Segmentation': 49, 'Medical Diagnosis': 344, 'Novelty Detection': 9, 'Online Clustering': 6, 'Split-MNIST': 6, 'Few-Shot Image Classification': 80, 'Novel Concepts': 13, 'multimodal generation': 9, 'Sentence-Pair Classification': 10, 'Video Captioning': 40, 'Few-Shot Learning': 583, 'Traffic Sign Recognition': 7, 'Motion Planning': 19, 'Image Captioning': 166, 'Change Point Detection': 7, 'BIG-bench Machine Learning': 196, 'Saliency Prediction': 5, 'Knowledge Graphs': 372, 'Relational Reasoning': 41, 'Scene Recognition': 18, 'Colorization': 37, 'Image-to-Image Translation': 518, 'Translation': 1415, 'Machine Translation': 987, 'Bilevel Optimization': 24, 'continual anomaly detection': 1, 'Unsupervised Anomaly Detection': 14, 'graph construction': 50, 'Split-CIFAR-10': 2, 'Instance Segmentation': 259, 'Memorization': 59, 'dialog state tracking': 11, 'Depth Estimation': 596, 'Monocular Depth Estimation': 246, 'Scene Understanding': 126, 'Unsupervised Monocular Depth Estimation': 23, 'Continual Named Entity Recognition': 3, 'named-entity-recognition': 535, 'Named Entity Recognition': 566, 'Continual Semantic Segmentation': 3, 'Code Generation': 83, 'GSM8K': 18, 'Instruction Following': 57, 'Mathematical Reasoning': 23, 'Point Processes': 10, 'Disentanglement': 538, 'Multiple Object Tracking': 13, 'Object Tracking': 64, 'Graph Generation': 48, 'Scene Graph Generation': 18, 'open-set classification': 5, 'Visual Question Answering': 210, 'Visual Question Answering (VQA)': 279, 'Scheduling': 41, 'Few-Shot Class-Incremental Learning': 6, 'DeepFake Detection': 18, 'Speech Synthesis': 46, 'Voice Conversion': 35, 'Novel View Synthesis': 54, 'Adversarial Robustness': 56, 'Scene Segmentation': 32, 'Denoising': 810, 'Video Denoising': 10, 'Density Estimation': 46, 'Data Poisoning': 23, 'Model Editing': 10, 'AutoML': 28, 'Edge-computing': 26, 'Image Retrieval': 230, 'Misinformation': 113, 'Management': 154, 'Lesion Segmentation': 427, 'Zero-Shot Learning': 163, 'Computed Tomography (CT)': 77, 'Zero-Shot Semantic Segmentation': 5, 'Feature Engineering': 109, 'Time Series': 150, 'Video Alignment': 4, 'Imitation Learning': 52, 'Activity Recognition': 38, 'Human Activity Recognition': 17, 'Chunking': 32, 'Action Assessment': 1, 'Action Quality Assessment': 4, 'Semi-Supervised Video Object Segmentation': 6, 'Visual Reasoning': 70, 'Q-Learning': 52, 'EEG': 93, 'Intent Detection': 34, 'slot-filling': 43, 'Slot Filling': 49, 'Task-Oriented Dialogue Systems': 21, 'Common Sense Reasoning': 79, '3D Scene Reconstruction': 12, '3D Reconstruction': 79, 'Graph Embedding': 222, 'Relation': 483, 'Personalized Federated Learning': 13, 'Food Recognition': 1, 'Quantum Machine Learning': 12, 'Out-of-Distribution Generalization': 36, 'Multi-class Classification': 47, 'Causal Inference': 26, 'Friction': 9, 'Document AI': 7, 'Document Image Classification': 15, 'Document Layout Analysis': 9, 'Key Information Extraction': 11, 'Relation Extraction': 472, 'Few-Shot Relation Classification': 3, 'Relation Classification': 57, 'Entity Typing': 30, 'Extractive Question-Answering': 26, 'Named Entity Recognition (NER)': 571, 'POS': 176, 'Relationship Extraction (Distant Supervised)': 3, 'Citation Intent Classification': 7, 'Dependency Parsing': 122, 'Medical Named Entity Recognition': 5, 'Participant Intervention Comparison Outcome Extraction': 2, 'Sentence Classification': 179, 'document understanding': 31, 'Semantic entity labeling': 4, 'Document-level Relation Extraction': 39, 'Semantic Role Labeling': 45, 'Dialog Relation Extraction': 4, 'Temporal Relation Extraction': 17, 'Time Series Analysis': 89, 'Time Series Forecasting': 25, 'Word Embeddings': 788, 'Joint Entity and Relation Extraction': 21, 'Text Segmentation': 12, 'Knowledge Base Population': 9, 'Position': 136, 'Entity Extraction using GAN': 7, 'Hierarchical Reinforcement Learning': 12, 'Sentence Embedding': 94, 'Sentence-Embedding': 89, 'Inductive Bias': 115, 'Clinical Concept Extraction': 4, 'Drug–drug Interaction Extraction': 4, 'Natural Language Inference': 420, 'Semantic Similarity': 375, 'Continual Relation Extraction': 9, 'Passage Retrieval': 69, 'Reading Comprehension': 498, 'Zero-shot Relation Classification': 5, 'Zero-shot Relation Triplet Extraction': 1, 'Masked Language Modeling': 65, 'Few-Shot Text Classification': 25, 'Prompt Engineering': 272, 'Zero-Shot Text Classification': 18, '4-ary Relation Extraction': 1, 'Knowledge Base Completion': 14, 'Semantic Composition': 9, 'Temporal Information Extraction': 6, 'TAG': 89, 'Medical Relation Extraction': 5, 'Open Information Extraction': 15, 'Image Augmentation': 208, 'Text Augmentation': 51, 'Term Extraction': 40, 'Vocal Bursts Intensity Prediction': 114, 'Hypernym Discovery': 3, 'Word Sense Disambiguation': 48, 'Entity Linking': 77, 'Vocal Bursts Type Prediction': 28, 'Temporal Relation Classification': 3, 'Negation': 35, 'Multi-Label Learning': 17, 'MULTI-VIEW LEARNING': 8, 'Link Prediction': 598, 'Machine Reading Comprehension': 181, 'Unsupervised Pre-training': 30, 'Event Extraction': 40, 'text-classification': 623, 'Text Classification': 737, 'Action Classification': 94, 'Action Understanding': 5, 'Video Understanding': 76, 'Human-Object Interaction Detection': 10, 'Specificity': 84, 'Toponym Recognition': 2, 'Graph Attention': 86, 'Event Argument Extraction': 8, 'Large Language Model': 495, 'Low Resource Named Entity Recognition': 8, 'Zero-shot Event Extraction': 2, 'Zero-shot Named Entity Recognition (NER)': 2, 'Event Relation Extraction': 4, 'Document-level RE with incomplete labeling': 2, 'Financial Relation Extraction': 4, 'Document Classification': 290, 'Semantic Textual Similarity': 463, 'entity_extraction': 1, 'Hyper-Relational Extraction': 1, 'Table Recognition': 4, 'Emotion Recognition in Conversation': 55, 'Word Sense Induction': 6, 'Relation Mapping': 2, 'Binary Relation Extraction': 1, 'PICO': 46, 'Sentence Similarity': 41, 'TriviaQA': 37, 'Attribute Extraction': 3, 'Cross-Domain Named Entity Recognition': 4, 'Entity Embeddings': 22, 'token-classification': 11, 'Token Classification': 11, 'Text Summarization': 446, 'Collision Avoidance': 7, 'Layout Design': 3, 'RTE': 9, 'Boundary Detection': 23, 'Definition Extraction': 8, 'Text Generation': 845, 'Cross-Modal Retrieval': 47, 'Recommendation Systems': 213, 'Relation Linking': 4, 'Entity Resolution': 12, 'Logical Reasoning': 61, 'Descriptive': 91, 'Learning-To-Rank': 35, 'Vocal Bursts Valence Prediction': 47, 'Cross-Domain Few-Shot': 23, 'Text Pair Classification': 2, 'intent-classification': 49, 'Intent Classification': 49, 'Intent Classification and Slot Filling': 4, 'Natural Language Understanding': 510, 'NER': 380, 'Image Reconstruction': 165, 'Cloze Test': 12, 'Triple Classification': 8, 'Knowledge Graph Completion': 84, 'Graph Clustering': 29, 'Collaborative Filtering': 42, 'Graph Classification': 450, 'Graph Regression': 42, 'Link Property Prediction': 5, 'Node Classification on Non-Homophilic (Heterophilic) Graphs': 4, 'Node Property Prediction': 9, 'Matrix Completion': 18, 'Knowledge Graph Embedding': 102, 'Click-Through Rate Prediction': 8, 'Music Recommendation': 5, 'Generative Adversarial Network': 250, 'Face Clustering': 6, 'Protein Function Prediction': 7, 'Open-Domain Question Answering': 267, 'Tensor Decomposition': 18, 'Temporal Knowledge Graph Completion': 6, 'Graph Reconstruction': 15, 'Imputation': 46, 'Temporal Sequences': 9, 'Dynamic Link Prediction': 11, 'Relational Pattern Learning': 1, 'Node Clustering': 34, 'Graph Similarity': 14, 'Inductive Link Prediction': 17, 'Graph Representation Learning': 422, 'Network Embedding': 317, 'Graph Mining': 22, 'Inductive knowledge graph completion': 4, 'Graph structure learning': 9, 'Knowledge Graph Embeddings': 172, 'graph partitioning': 7, 'Dynamic Node Classification': 2, 'Fraud Detection': 22, 'Sequential Recommendation': 22, 'Traffic Prediction': 9, 'Edge Classification': 10, 'feature selection': 72, 'Optical Character Recognition': 33, 'Optical Character Recognition (OCR)': 79, 'Generalization Bounds': 16, 'Counterfactual Explanation': 9, 'Riddle Sense': 2, 'Dynamic graph embedding': 5, 'Complex Query Answering': 5, 'Stochastic Block Model': 5, 'Backdoor Attack': 31, 'Entity Typing on DH-KGs': 1, 'Link prediction on DH-KGs': 1, 'Text Detection': 20, 'Entity Alignment': 12, 'Navigate': 61, 'Disease Prediction': 13, 'Ensemble Learning': 63, 'Type prediction': 11, 'Synthetic Data Generation': 44, 'Time-interval Prediction': 1, 'Hyperedge Prediction': 1, 'Argument Mining': 14, 'AMR-to-Text Generation': 9, 'Predicate Classification': 3, 'Edge Detection': 19, 'Extractive Summarization': 67, 'Data Visualization': 9, 'Sociology': 7, 'Linguistic Acceptability': 17, 'Multimodal Intent Recognition': 3, 'Multi-task Language Understanding': 12, 'Hallucination': 115, 'Paraphrase Identification': 32, 'Cross-Lingual Natural Language Inference': 13, 'Cross-Lingual Paraphrase Identification': 3, 'Bias Detection': 16, 'Medical Visual Question Answering': 7, 'Logical Reasoning Reading Comprehension': 3, 'MRPC': 5, 'QNLI': 5, 'QQP': 9, 'Arithmetic Reasoning': 28, 'Multi-hop Question Answering': 25, 'text similarity': 27, 'Ontology Subsumption Inferece': 1, 'Fact Verification': 31, 'Response Generation': 50, 'Clinical Knowledge': 10, 'XLM-R': 95, 'Weakly-supervised Learning': 47, 'Re-Ranking': 71, 'Multiple-choice': 102, 'Stance Detection': 28, 'Abstractive Text Summarization': 194, 'Document Summarization': 230, 'Weakly Supervised Classification': 4, 'Humanitarian': 10, 'Answer Selection': 82, 'Sentence Pair Modeling': 5, 'Chatbot': 56, 'Intent Discovery': 5, 'Distributed Computing': 6, 'Word Translation': 20, 'FAD Curve Analysis': 1, 'Systematic Generalization': 18, 'Hard-label Attack': 1, 'Cross-Lingual Transfer': 302, 'Hate Speech Detection': 286, 'Generalized Few-Shot Learning': 3, 'Goal-Oriented Dialog': 4, 'Spoken Language Understanding': 36, 'Task 2': 47, 'Multi Label Text Classification': 73, 'Multi-Label Text Classification': 90, 'Sentence Completion': 10, 'Timeline Summarization': 2, 'Overall - Test': 3, 'Evidence Selection': 2, 'NMT': 191, 'Causal Judgment': 2, 'Date Understanding': 5, 'Disambiguation QA': 2, 'Formal Fallacies Syllogisms Negation': 2, 'Hyperbaton': 2, 'Movie Recommendation': 5, 'Multiple Choice Question Answering (MCQA)': 23, 'Penguins In A Table': 2, 'Reasoning About Colored Objects': 2, 'Ruin Names': 2, 'Sarcasm Detection': 25, 'SNARKS': 2, 'Sports Understanding': 2, 'Unsupervised Opinion Summarization': 1, 'Rumour Detection': 10, 'Text Retrieval': 79, 'News Classification': 15, 'Adversarial Text': 12, 'Visual Commonsense Reasoning': 12, 'Text Matching': 36, 'Zero-Shot Cross-Lingual Transfer': 62, 'Paraphrase Generation': 16, 'Adversarial Defense': 20, 'Low-rank compression': 1, 'Knowledge Probing': 7, 'Conditional Text Generation': 41, 'Dialogue Generation': 50, 'Story Completion': 9, 'Question Generation': 92, 'Question-Generation': 87, 'Text Simplification': 41, 'Data-to-Text Generation': 136, 'Sentence Embeddings': 295, 'Part-Of-Speech Tagging': 288, 'Robust classification': 4, 'Sound Event Detection': 7, 'Video Quality Assessment': 37, 'Morphological Tagging': 9, 'Lemmatization': 41, 'Video Question Answering': 67, 'Chinese Named Entity Recognition': 16, 'Morphological Analysis': 25, 'Word Similarity': 60, 'Unsupervised Part-Of-Speech Tagging': 2, 'LEMMA': 12, 'Sentence Compression': 16, 'Probabilistic Programming': 4, 'Transition-Based Dependency Parsing': 2, 'Marketing': 21, 'Predicate Detection': 3, 'Semantic Role Labeling (predicted predicates)': 2, 'Constituency Parsing': 17, 'Sentence segmentation': 12, 'Art Analysis': 4, 'Decipherment': 6, 'Chinese Word Segmentation': 17, 'Arabic Text Diacritization': 2, 'Text-To-Speech Synthesis': 14, 'Keyword Extraction': 13, 'Cultural Vocal Bursts Intensity Prediction': 23, 'Polyphone disambiguation': 3, 'FLUE': 1, 'Open-Ended Question Answering': 366, 'Grammatical Error Detection': 13, 'text annotation': 10, 'Selection bias': 15, 'Commonsense Causal Reasoning': 1, 'Face Model': 26, 'Dialogue Understanding': 7, 'Intent Recognition': 13, 'Language Identification': 52, 'Lexical Analysis': 4, 'Morphological Inflection': 9, 'Vietnamese Word Segmentation': 2, 'Cross-Lingual POS Tagging': 1, 'Semantic Parsing': 379, 'CCG Supertagging': 2, 'Grammatical Error Correction': 241, 'Stock Prediction': 4, 'Lexical Simplification': 7, 'Vietnamese Parsing': 1, 'Lexical Normalization': 6, 'Morphological Disambiguation': 3, 'Keyphrase Extraction': 12, 'Embeddings Evaluation': 2, 'Syntax Representation': 3, 'Deception Detection': 3, 'Fake News Detection': 316, 'Computer Security': 4, 'Ad-Hoc Information Retrieval': 8, 'Image Similarity Search': 4, 'Authorship Verification': 5, 'Clickbait Detection': 3, 'Subjectivity Analysis': 10, 'Web Credibility': 1, 'Twitter Bot Detection': 2, 'Comment Generation': 2, 'Claim Verification': 9, 'News Recommendation': 15, 'Factual Visual Question Answering': 2, 'Image-guided Story Ending Generation': 4, 'Phrase Grounding': 5, 'Science Question Answering': 12, 'Neural Network Compression': 4, 'Intrusion Detection': 13, 'Image-text matching': 21, 'Style Transfer': 152, 'Satire Detection': 3, 'Phishing Website Detection': 1, 'Reliable Intelligence Identification': 1, 'Pseudo Label': 87, 'Stance Classification': 5, 'Feature Importance': 15, 'Self-Supervised Image Classification': 31, 'Dichotomous Image Segmentation': 13, 'Face Alignment': 24, 'Thermal Image Segmentation': 28, '2D Human Pose Estimation': 5, 'Keypoint Detection': 14, 'Multi-Person Pose Estimation': 7, 'Pose Tracking': 3, 'Facial Landmark Detection': 9, 'Person Re-Identification': 304, 'Self-Supervised Person Re-Identification': 4, 'Semi-Supervised Image Classification': 19, 'Multilingual NLP': 40, 'Poker Hand Classification': 1, 'Pose Prediction': 8, 'Riemannian optimization': 6, 'Mutual Information Estimation': 10, 'Referring Expression': 19, 'Referring Expression Comprehension': 15, 'Visual Entailment': 10, 'Zero-Shot Cross-Modal Retrieval': 7, 'Face Identification': 34, 'Time Series Clustering': 3, 'Unsupervised Object Segmentation': 4, 'Cyber Attack Detection': 1, 'Network Intrusion Detection': 8, 'Time Series Classification': 12, 'Time Series Regression': 3, 'Video Classification': 30, 'Action Recognition': 476, 'Self-Supervised Image Classification (with CLIP)': 1, 'Temporal Action Localization': 575, 'Acoustic Unit Discovery': 3, 'Speaker Verification': 29, 'Deep Clustering': 12, 'Image Clustering': 12, 'Cross-Modal Person Re-Identification': 4, 'Citation Prediction': 4, 'Vehicle Re-Identification': 17, 'Long-tail Learning': 8, 'Image-to-Text Retrieval': 10, 'Open Vocabulary Attribute Detection': 3, 'Semi-Supervised Text Classification': 25, 'Video Generation': 53, 'Video Prediction': 17, 'Visual Analogies': 2, '3D Shape Representation': 3, 'Single-View 3D Reconstruction': 3, 'Multivariate Time Series Forecasting': 3, 'Probabilistic Time Series Forecasting': 1, 'Scene Text Recognition': 12, 'Video Recognition': 34, 'Video Retrieval': 65, 'Zero-Shot Transfer Image Classification': 4, 'Self-Supervised Action Recognition': 5, 'Image Colorization': 16, 'Starcraft': 15, 'Materials Screening': 1, 'Transductive Learning': 11, 'Survival Analysis': 6, 'Time-to-Event Prediction': 3, 'Moment Retrieval': 11, 'Video Corpus Moment Retrieval': 1, 'Graph Property Prediction': 16, 'Molecular Property Prediction': 18, 'molecular representation': 11, 'Point Cloud Pre-training': 3, 'Deblurring': 64, 'Image Defocus Deblurring': 6, 'Image Dehazing': 13, 'Image Denoising': 143, 'Image Restoration': 146, 'Voice Similarity': 1, 'Multi-Label Image Classification': 9, 'Skeleton Based Action Recognition': 63, 'Meta Reinforcement Learning': 23, 'Text Spotting': 3, 'Image Deblurring': 30, 'Rain Removal': 23, 'Single Image Deraining': 8, 'Single Image Desnowing': 1, 'Font Generation': 7, 'LIDAR Semantic Segmentation': 17, 'Incomplete multi-view clustering': 1, 'Time Series Prediction': 8, 'Music Information Retrieval': 45, 'Sleep Staging': 2, 'Zero-Shot Instance Segmentation': 2, 'Point Cloud Registration': 16, 'Brain Segmentation': 17, 'Super-Resolution': 520, 'EEG Denoising': 2, 'Electroencephalogram (EEG)': 58, '3D Anomaly Detection': 1, 'Unsupervised Image Classification': 3, 'Highlight Detection': 6, 'Natural Language Moment Retrieval': 3, 'Relationship Detection': 3, 'Visual Relationship Detection': 3, 'Open-World Semi-Supervised Learning': 2, 'Surface Normal Estimation': 13, 'Feature Compression': 3, 'Multiple Instance Learning': 38, 'Video Anomaly Detection': 4, 'Copy Detection': 4, 'Patch Matching': 4, 'Off-policy evaluation': 6, 'Dynamic Time Warping': 15, 'Zero-shot Slot Filling': 4, '3D Object Classification': 5, 'Code Search': 10, 'Session-Based Recommendations': 6, 'Automated Feature Engineering': 5, 'Efficient Exploration': 14, 'Optical Flow Estimation': 113, 'Generic Event Boundary Detection': 1, 'Real-Time Semantic Segmentation': 42, '3D Point Cloud Classification': 14, 'Few-Shot 3D Point Cloud Classification': 7, 'Hyperspectral Image Classification': 8, 'audio-visual learning': 3, 'Panoptic Segmentation': 68, 'Video Super-Resolution': 49, 'Audio-Visual Speech Recognition': 3, 'Visual Speech Recognition': 6, 'Traveling Salesman Problem': 6, 'Universal Segmentation': 5, 'Multimodal Recommendation': 1, 'Semantic correspondence': 11, 'Action Segmentation': 14, 'Speaker Identification': 13, 'Speech Enhancement': 17, 'Depth Prediction': 122, 'Visual Navigation': 9, 'Cell Segmentation': 129, 'Data Compression': 11, 'Dictionary Learning': 20, 'Temporal Localization': 26, 'Graph Anomaly Detection': 3, 'Point Cloud Classification': 11, 'Causal Discovery': 8, 'Binarization': 19, 'Visual Grounding': 33, 'Visual Localization': 9, 'Long-range modeling': 39, 'Image Enhancement': 50, 'MRI Reconstruction': 22, 'Part-aware Panoptic Segmentation': 3, 'Neural Rendering': 24, 'Action Anticipation': 3, 'Long Term Action Anticipation': 1, 'Generalized Zero-Shot Learning': 10, 'Aspect Category Detection': 12, 'Breast Cancer Histology Image Classification': 2, 'Medical Image Retrieval': 5, 'Scene Classification': 15, 'Multi-Object Tracking': 32, 'Protein-Ligand Affinity Prediction': 1, 'Graph Matching': 23, 'Video Editing': 17, 'Conversation Disentanglement': 4, 'Weather Forecasting': 10, 'De-identification': 10, 'Hyperparameter Optimization': 42, 'Robust Speech Recognition': 2, 'Unsupervised Image-To-Image Translation': 55, 'Robust Object Detection': 17, 'Conversational Response Selection': 5, 'Music Generation': 11, 'Gaze Estimation': 10, 'Self-Learning': 29, '3D Face Alignment': 3, '3D Face Animation': 6, '3D Face Modelling': 5, '3D Face Reconstruction': 23, 'Face Reconstruction': 29, 'Scene Generation': 18, 'Resynthesis': 4, 'Motion Synthesis': 11, 'Image Manipulation': 44, 'Data Free Quantization': 1, 'Additive models': 5, 'Predict Future Video Frames': 1, 'Unsupervised Object Detection': 1, 'Generalizable Person Re-identification': 15, 'Single Image Dehazing': 5, '3D Pose Estimation': 4, 'Homography Estimation': 3, 'imbalanced classification': 13, 'Mortality Prediction': 7, 'Autonomous Navigation': 25, 'SSIM': 61, 'Audio Generation': 11, 'Person Identification': 7, 'Odd One Out': 2, 'Motion Disentanglement': 4, 'Survival Prediction': 16, 'Cross-Domain Text Classification': 8, 'Point Cloud Generation': 8, 'Text-to-Video Generation': 10, 'Inverse Rendering': 8, 'Image Super-Resolution': 235, 'Hyperspectral Unmixing': 2, 'Audio captioning': 8, 'Shadow Detection': 6, 'Pedestrian Attribute Recognition': 6, 'Optic Disc Segmentation': 7, 'Depression Detection': 8, 'Compositional Zero-Shot Learning': 2, 'Object Reconstruction': 6, 'Novel Class Discovery': 7, 'Sign Language Recognition': 8, 'Face Anti-Spoofing': 35, 'Multi-target Domain Adaptation': 5, 'Self-Driving Cars': 30, 'Crowd Counting': 9, 'Music Source Separation': 4, 'Face Reenactment': 10, 'blind source separation': 4, 'Empathetic Response Generation': 2, 'Lesion Classification': 24, 'Skin Lesion Classification': 14, 'Monocular 3D Object Detection': 22, 'Image Categorization': 2, 'Image Registration': 27, 'Text to 3D': 20, 'text-to-3d-human': 1, 'Disaster Response': 3, 'Unity': 9, 'Video Segmentation': 21, 'Human Parsing': 14, 'Virtual Try-on': 11, 'Multi-View 3D Reconstruction': 1, 'Numerical Integration': 3, 'Zero-shot Text-to-Video Generation': 1, 'Auxiliary Learning': 5, 'Speaker Recognition': 18, '3D Human Pose Estimation': 15, 'Multi-agent Reinforcement Learning': 32, 'Starcraft II': 10, 'Image Compression': 31, 'Metal Artifact Reduction': 2, 'Style Generalization': 1, 'Lip Reading': 7, 'Human motion prediction': 3, 'motion prediction': 34, 'Image Matting': 14, 'One-Shot Face Stylization': 1, 'Talking Head Generation': 11, 'Video Frame Interpolation': 10, 'Video-Based Person Re-Identification': 16, 'Emotion Recognition': 673, 'Speech Emotion Recognition': 154, 'Head Pose Estimation': 8, 'Linear-Probe Classification': 3, 'STS': 64, 'Cross-Lingual Bitext Mining': 4, 'Cross-Lingual Document Classification': 13, 'Joint Multilingual Sentence Representations': 1, 'Parallel Corpus Mining': 4, 'Sentence Embeddings For Biomedical Texts': 1, 'Humor Detection': 3, 'Community Question Answering': 124, 'Question Similarity': 25, 'Reverse Dictionary': 3, 'Service Composition': 2, 'Topological Data Analysis': 21, 'Open Intent Discovery': 1, 'Dialogue Evaluation': 8, 'Unsupervised Extractive Summarization': 8, '3D Question Answering (3D-QA)': 2, 'Argument Retrieval': 3, 'Biomedical Information Retrieval': 4, 'Duplicate-Question Retrieval': 3, 'Entity Retrieval': 13, 'News Retrieval': 3, 'Tweet Retrieval': 2, 'Zero-shot Text Search': 6, 'Semantic Retrieval': 10, 'Text Clustering': 13, 'Avg': 4, 'Speech-to-Text Translation': 13, 'Story Continuation': 3, 'Implicit Relations': 7, 'Blocking': 16, 'Short Text Clustering': 2, 'Automated Essay Scoring': 2, 'Logical Sequence': 3, 'Sentence Ordering': 8, 'Dialect Identification': 10, 'Zero-shot Audio Classification': 2, 'Word Alignment': 36, 'Document Level Machine Translation': 3, 'Matrix Factorization / Decomposition': 1, 'EDIT Task': 1, 'knowledge editing': 7, 'Open Knowledge Graph Embedding': 1, 'Ontology Matching': 4, 'Multi-modal Entity Alignment': 2, 'Explainable Recommendation': 5, 'Product Recommendation': 12, 'Open World Object Detection': 5, 'Entity Disambiguation': 11, 'Document Embedding': 46, 'Citation Recommendation': 1, 'Activity Prediction': 6, 'Keyphrase Generation': 5, 'Hierarchical Multi-label Classification': 4, 'Text Categorization': 138, 'Learning Semantic Representations': 17, 'SQL-to-Text': 1, 'Superpixel Image Classification': 1, 'Stock Market Prediction': 5, 'Ensemble Pruning': 1, 'Combinatorial Optimization': 32, 'set matching': 5, 'Graph Sampling': 7, 'Inference Attack': 9, 'Membership Inference Attack': 6, 'Trajectory Prediction': 28, 'Visual Tracking': 4, 'Scene Graph Classification': 4, 'Bayesian Optimisation': 8, 'Reconstruction Attack': 5, 'Two-sample testing': 5, 'Structual Feature Correlation': 1, 'Eeg Decoding': 2, 'Protein Design': 8, '3D Object Recognition': 3, 'Malware Classification': 2, 'Weakly supervised segmentation': 14, 'Code Classification': 3, 'Supervised Video Summarization': 2, 'Video Summarization': 8, 'Scene Graph Detection': 3, 'C++ code': 4, 'Atomic Forces': 1, 'RGB Salient Object Detection': 5, 'Salient Object Detection': 28, 'Tensor Networks': 2, 'Learning with noisy labels': 7, 'Image Classification with Differential Privacy': 2, 'Privacy Preserving Deep Learning': 6, 'Medical Image Generation': 14, 'Evolutionary Algorithms': 12, 'Diabetic Retinopathy Detection': 6, 'Acne Severity Grading': 2, 'Semi-supervised Medical Image Classification': 9, 'COVID-19 Diagnosis': 16, 'Collaborative Inference': 2, 'Lung Disease Classification': 3, 'Partial Label Learning': 8, 'Cell Detection': 15, 'severity prediction': 4, 'Medical Object Detection': 8, 'medical image detection': 5, 'Medical Image Registration': 6, 'Diabetic Retinopathy Grading': 6, 'Lesion Detection': 28, 'Conformal Prediction': 8, 'Skin Cancer Classification': 4, 'Texture Classification': 5, 'One-Class Classification': 4, 'Pneumonia Detection': 6, 'Image Classification with Label Noise': 1, 'Cancer-no cancer per image classification': 1, 'UNET Segmentation': 4, 'Breast Cancer Detection': 8, 'Content-Based Image Retrieval': 10, 'Plant Phenotyping': 55, 'Object Counting': 11, 'Crop Classification': 3, 'Crop Yield Prediction': 2, 'Monocular Reconstruction': 6, 'Cloud Computing': 6, 'GPR': 2, 'Head Detection': 1, 'Camera Calibration': 11, 'Occlusion Handling': 3, 'Morphology classification': 22, 'Astronomy': 14, 'Photometric Redshift Estimation': 1, 'Universal Domain Adaptation': 8, 'Sperm Morphology Classification': 1, 'Classifier calibration': 22, 'Fine-Grained Image Recognition': 6, 'Surface Normals Estimation': 6, 'Weakly-Supervised Object Localization': 6, 'Small Data Image Classification': 7, 'Cell Tracking': 12, 'Multi-tissue Nucleus Segmentation': 4, 'Pancreas Segmentation': 8, 'Skin Cancer Segmentation': 8, 'Video Polyp Segmentation': 5, 'Retinal OCT Disease Classification': 2, 'Volumetric Medical Image Segmentation': 14, 'Cardiac Segmentation': 12, 'Visual Prompting': 11, 'Unsupervised Image Segmentation': 9, '3D Medical Imaging Segmentation': 11, 'Speech Separation': 3, 'Skull Stripping': 4, 'One-Shot Segmentation': 6, 'Left Atrium Segmentation': 5, 'Brain Image Segmentation': 7, 'Skin Lesion Segmentation': 126, 'Colorectal Polyps Characterization': 3, 'Polyp Segmentation': 1, 'Camouflaged Object Segmentation': 7, 'Handwriting Recognition': 5, 'Handwritten Text Recognition': 5, 'One-shot visual object segmentation': 2, 'Visual Object Tracking': 5, 'Lung Nodule Segmentation': 6, 'Multi-modal image segmentation': 2, 'Referring Expression Segmentation': 13, 'Referring Image Matting (Expression-based)': 1, 'Referring Image Matting (Keyword-based)': 1, 'Referring Image Matting (RefMatte-RW100)': 1, 'Zero Shot Segmentation': 15, 'Video Instance Segmentation': 9, 'Semi-Supervised Semantic Segmentation': 10, 'Weakly-Supervised Semantic Segmentation': 20, 'Breast Tumour Classification': 3, 'Colorectal Gland Segmentation:': 1, 'Retinal Vessel Segmentation': 11, 'Vessel Detection': 5, 'Deep Attention': 8, 'Open Vocabulary Semantic Segmentation': 10, '3D Architecture': 4, 'Contour Detection': 5, 'Iris Recognition': 3, 'Steering Control': 1, 'Medical X-Ray Image Segmentation': 1, 'Unsupervised Semantic Segmentation': 10, 'Spinal Cord Gray Matter - Segmentation': 1, 'UNET Quantization': 1, 'Change detection for remote sensing images': 2, 'Lake Detection': 1, 'Lake Ice Monitoring': 1, 'Remote Sensing Image Classification': 2, 'Segmentation Of Remote Sensing Imagery': 5, 'The Semantic Segmentation Of Remote Sensing Imagery': 2, 'Webcam (RGB) image classification': 1, 'Heart Segmentation': 2, 'Semi-supervised Medical Image Segmentation': 40, 'Unsupervised Instance Segmentation': 1, 'Unsupervised Panoptic Segmentation': 1, 'Unsupervised Zero-Shot Instance Segmentation': 1, 'Unsupervised Zero-Shot Panoptic Segmentation': 1, 'Referring Video Object Segmentation': 5, 'Constrained Clustering': 3, 'highlight removal': 1, 'Specular Reflection Mitigation': 1, 'Road Segmentation': 8, 'Liver Segmentation': 14, 'MS-SSIM': 6, 'Semi-Supervised Domain Generalization': 2, 'Open Vocabulary Object Detection': 21, 'Open Vocabulary Panoptic Segmentation': 1, 'Zero-Shot Image Classification': 11, 'Unsupervised Object Localization': 1, 'MRI segmentation': 16, 'Point Tracking': 3, 'Vision-Language Segmentation': 2, '3D Object Reconstruction': 2, 'Learning with coarse labels': 1, 'Weakly-supervised instance segmentation': 4, 'Line Detection': 5, 'Online Domain Adaptation': 2, 'Noise Estimation': 13, 'Disparity Estimation': 9, 'Cloud Detection': 3, 'Crop Type Mapping': 1, 'Vector Graphics': 3, 'Multiview Learning': 2, 'MORPH': 13, 'Robot Navigation': 19, 'X-Ray Diffraction (XRD)': 3, 'Lane Detection': 29, 'Defect Detection': 10, 'Image Shadow Removal': 5, 'Shadow Removal': 8, 'Object Discovery': 11, 'Region Proposal': 26, 'Weakly supervised Semantic Segmentation': 18, 'Scene Parsing': 169, 'Street Scene Parsing': 1, 'Material Classification': 2, 'Material Recognition': 3, '2D Semantic Segmentation': 29, 'Camera Auto-Calibration': 1, 'Scene Labeling': 8, 'Land Cover Classification': 5, 'Real-Time Object Detection': 50, 'Room Layout Estimation': 2, 'Sensor Fusion': 16, 'Visual Odometry': 15, 'Visual Social Relationship Recognition': 1, '3D Instance Segmentation': 13, 'Wireframe Parsing': 1, 'Aerial Scene Classification': 2, 'Simultaneous Localization and Mapping': 14, 'Photo Retouching': 3, 'Saliency Detection': 9, 'Face Parsing': 7, '3D Semantic Scene Completion': 6, 'Real-Time 3D Semantic Segmentation': 1, 'Semantic Segmentation on ScanNet': 1, 'Building change detection for remote sensing images': 2, 'Extracting Buildings In Remote Sensing Images': 1, 'Reflection Removal': 61, 'image smoothing': 3, 'Color Constancy': 3, 'Intrinsic Image Decomposition': 4, 'Low-Rank Matrix Completion': 6, 'Foreground Segmentation': 69, 'Defocus Blur Detection': 3, 'Image Manipulation Detection': 5, 'Video Background Subtraction': 3, 'Zero-Shot Object Detection': 4, 'Transparent objects': 4, 'Unsupervised Video Object Segmentation': 2, 'Real-time Instance Segmentation': 9, '3D Part Segmentation': 11, 'Depth Completion': 28, 'Spacecraft Pose Estimation': 2, '3D Shape Reconstruction From A Single 2D Image': 1, 'Image Retargeting': 2, 'Mixed Reality': 3, 'Hand Segmentation': 2, 'Human Detection': 12, 'Interest Point Detection': 2, 'Class-Incremental Semantic Segmentation': 1, 'Overlapped 100-10': 1, 'Overlapped 100-5': 1, 'Overlapped 100-50': 1, 'Overlapped 14-1': 1, 'Overlapped 50-50': 1, 'Overlapped 5-3': 1, 'Setting-1/4': 1, 'Twitter Sentiment Analysis': 10, 'Cross-Lingual Sentiment Classification': 5, 'Data Summarization': 3, 'Visual Sentiment Prediction': 1, 'Stock Trend Prediction': 1, 'Multimodal Sentiment Analysis': 32, 'Aspect Extraction': 25, 'Extract Aspect': 9, 'Aspect-Based Sentiment Analysis': 204, 'Algorithmic Trading': 2, 'Zero-shot Sentiment Classification': 2, 'Stock Price Prediction': 4, 'Abuse Detection': 5, 'Model Poisoning': 4, 'backdoor defense': 5, 'Epidemiology': 8, 'Linguistic steganography': 1, 'Data-free Knowledge Distillation': 3, 'SST-2': 9, 'Ethics': 16, 'Video Description': 5, 'Arabic Sentiment Analysis': 1, 'Aspect-Based Sentiment Analysis (ABSA)': 223, 'Aspect Sentiment Triplet Extraction': 18, 'target-oriented opinion words extraction': 5, 'UIE': 2, 'Aspect Category Polarity': 3, 'Persian Sentiment Analysis': 2, 'Sentiment Dependency Learning': 1, 'Aspect-Category-Opinion-Sentiment Quadruple Extraction': 1, 'Aspect-oriented Opinion Extraction': 2, 'Extract aspect-polarity tuple': 1, 'Topic Models': 329, 'Aspect-Sentiment-Opinion Triplet Extraction': 1, 'Aspect Term Extraction and Sentiment Classification': 4, 'Cross-Lingual Word Embeddings': 29, 'Unsupervised Machine Translation': 67, 'Bilingual Lexicon Induction': 63, 'Unsupervised Speech Recognition': 2, 'Diachronic Word Embeddings': 4, 'Zero-Shot Composed Image Retrieval (ZS-CIR)': 2, 'Political Salient Issue Orientation Detection': 1, 'Cross-Lingual Information Retrieval': 7, 'Nutrition': 6, 'Connective Detection': 1, 'Discourse Parsing': 6, 'Discourse Segmentation': 2, 'Contextualised Word Representations': 1, 'Playing the Game of 2048': 3, 'Readmission Prediction': 3, 'Topic coverage': 4, 'Unsupervised Text Classification': 5, 'Proper Noun': 1, 'Document Ranking': 18, 'Abstractive Dialogue Summarization': 9, 'Goal-Oriented Dialogue Systems': 4, 'Explainable Models': 4, 'Emotional Intelligence': 12, 'Text Anonymization': 1, '3D Shape Retrieval': 2, 'Image Stylization': 4, 'Foveation': 1, 'Audio Synthesis': 2, 'Fake Image Detection': 2, 'D4RL': 19, 'Offline RL': 61, 'Diffusion Personalization': 2, 'Texture Synthesis': 19, 'Text-based Image Editing': 5, 'Viewpoint Estimation': 3, '10-shot image generation': 2, 'Image to 3D': 5, 'Image to Video Generation': 5, 'Cloud Removal': 1, 'Handwritten Word Generation': 1, 'Scene Text Detection': 4, 'single-image-generation': 2, 'Video Object Detection': 8, 'Blind Face Restoration': 5, 'text-guided-image-editing': 7, 'Board Games': 7, 'Scene Text Editing': 1, 'Cross-lingual Text-to-Image Generation': 1, 'Virtual Try-on (Model2Street)': 1, 'Virtual Try-on (Shop2Street)': 1, 'Virtual Try-on (Street2Street)': 1, 'Image Quality Assessment': 33, 'No-Reference Image Quality Assessment': 7, 'Video Inpainting': 12, 'Consistent Character Generation': 1, 'Story Visualization': 4, 'Audio Denoising': 4, 'Perceptual Distance': 4, 'Multimodal Machine Translation': 63, 'Blind Super-Resolution': 16, 'Robotic Grasping': 3, 'Image Animation': 1, 'Figurative Language Visualization': 1, '3D scene Editing': 3, 'Motion Estimation': 30, 'Fashion Synthesis': 4, 'Unconditional Image Generation': 12, 'Image Morphing': 6, 'Operator learning': 5, 'Stereo Matching': 24, '3D Shape Generation': 5, 'Synthetic-to-Real Translation': 28, 'Hand Pose Estimation': 7, 'Multimodal Unsupervised Image-To-Image Translation': 10, 'Real-to-Cartoon translation': 1, 'JPEG Decompression': 1, 'Uncropping': 2, 'Bird View Synthesis': 1, 'Cross-View Image-to-Image Translation': 3, 'DreamBooth Personalized Generation': 1, 'Font Style Transfer': 1, 'Typeface Completion': 1, 'Face Age Editing': 2, 'Human Aging': 1, 'Caricature': 5, 'Sketch-to-Image Translation': 4, 'Supervised Anomaly Detection': 3, 'Weakly-supervised Anomaly Detection': 1, 'Parameter Prediction': 2, 'Face Sketch Synthesis': 4, 'Photo-To-Caricature Translation': 1, 'Splenomegaly Segmentation On Multi-Modal Mri': 1, 'Video Temporal Consistency': 1, 'Stereo Matching Hand': 10, 'Sensor Modeling': 3, 'Rgb-T Tracking': 1, 'Image Stitching': 5, 'Facial Expression Translation': 1, 'Gesture-to-Gesture Translation': 1, 'Snow Removal': 3, 'Low-Light Image Enhancement': 12, 'Generalizable Novel View Synthesis': 1, 'Gournd video synthesis from satellite image': 1, \"Alzheimer's Disease Detection\": 37, 'Image Relighting': 5, 'Video Style Transfer': 2, 'Classification Of Breast Cancer Histology Images': 1, 'Age-Invariant Face Recognition': 6, 'Kinship Verification': 3, 'Talking Face Generation': 60, 'Infrared And Visible Image Fusion': 5, 'Low-light Pedestrian Detection': 1, 'Multispectral Object Detection': 1, 'Pedestrian Detection': 23, 'Thermal Infrared Pedestrian Detection': 1, 'Person Recognition': 4, 'Underwater Image Restoration': 3, 'Depth Map Super-Resolution': 3, 'Color Manipulation': 1, 'Protein Structure Prediction': 6, 'Uncertainty Visualization': 2, 'SVBRDF Estimation': 1, 'Density Ratio Estimation': 1, 'Geophysics': 2, 'Visual Place Recognition': 11, 'Efficient Neural Network': 9, 'Image Outpainting': 12, 'Image Inpainting': 424, 'Seeing Beyond the Visible': 3, 'Compressive Sensing': 13, 'Video-to-Video Synthesis': 4, 'Image Compressed Sensing': 2, 'JPEG Artifact Removal': 3, 'Image Cropping': 7, 'Unified Image Restoration': 1, 'Traffic Data Imputation': 1, 'Image-Variation': 3, 'Packet Loss Concealment': 1, '3D Human Shape Estimation': 1, 'Learning Representation Of Multi-View Data': 1, 'Lightfield': 1, 'Audio inpainting': 1, 'Conditional Image Generation': 146, 'Unsupervised Facial Landmark Detection': 2, 'Relation Network': 18, 'Keypoint Estimation': 5, 'Face Generation': 261, 'Face Detection': 35, 'Steganalysis': 2, 'Robust Face Recognition': 16, 'Constrained Lip-synchronization': 2, 'Face to Face Translation': 2, 'Text-to-Face Generation': 8, 'Gender Bias Detection': 2, 'Micro-Expression Recognition': 6, 'Landmark Tracking': 1, 'Facial Editing': 2, 'Unconditional Video Generation': 1, 'Kinship face generation': 1, 'Brain Morphometry': 1, 'Audio-Visual Synchronization': 3, 'gaze redirection': 1, 'Facial Beauty Prediction': 2, 'Video Compression': 8, 'Heterogeneous Face Recognition': 7, 'Face Hallucination': 5, 'Counterfactual Inference': 3, 'Face Transfer': 1, 'Facial expression generation': 2, 'Image Harmonization': 63, 'Inference Optimization': 2, 'Video Harmonization': 1, 'Lung Cancer Diagnosis': 1, '3D-Aware Image Synthesis': 34, 'Bone Suppression From Dual Energy Chest X-Rays': 1, 'Indoor Scene Synthesis': 1, 'Facial Inpainting': 35, 'Layout-to-Image Generation': 19, 'ROI-based image generation': 1, 'Image Generation from Scene Graphs': 7, 'Pose-Guided Image Generation': 4, 'Hallucination Evaluation': 3, 'Causal Language Modeling': 5, 'Handwriting generation': 3, 'Person Search': 11, 'Text based Person Search': 6, 'Long Form Question Answering': 15, 'Answer Generation': 40, 'Motion Captioning': 1, 'Motion Segmentation': 6, 'CyberBattleSim': 1, 'CyberBattleSim (RL) chain scenario': 1, 'NetSecGame': 1, 'NetSecGame (RL) full scenario defender': 1, 'NetSecGame (RL) full scenario no defender': 1, 'NetSecGame (RL) small scenario defender': 1, 'NetSecGame (RL) small scenario no defender': 1, 'Text2text Generation': 1, 'Adversarial Attack Detection': 2, 'KG-to-Text Generation': 8, 'Temporal/Casual QA': 2, 'OpenAI Gym': 16, 'Passage Ranking': 14, 'Genre classification': 10, 'Knowledge Tracing': 2, 'Style change detection': 1, 'Recipe Generation': 3, 'Literature Mining': 3, 'Definition Modelling': 1, 'Zero-Shot Video Question Answer': 19, 'CoLA': 9, 'IUPAC Name Prediction': 1, 'Molecule Captioning': 1, 'Lay Summarization': 1, 'News Summarization': 19, 'Persuasiveness': 2, 'Code Summarization': 13, 'KB-to-Language Generation': 2, 'Graph-to-Sequence': 8, 'Spoken Dialogue Systems': 2, 'Referring expression generation': 5, 'Sentence Fusion': 13, 'Zero-shot Text Retrieval': 1, 'Cross-Lingual Abstractive Summarization': 4, 'Extreme Summarization': 6, 'Multi-Document Summarization': 183, 'Key Point Matching': 2, 'Extractive Text Summarization': 31, 'Query-focused Summarization': 10, 'Review Generation': 4, 'Scientific Document Summarization': 2, 'coreference-resolution': 3, 'Coreference Resolution': 4, 'Extractive Document Summarization': 6, 'Reader-Aware Summarization': 1, 'Story Generation': 128, 'Event Expansion': 2, 'Event Causality Identification': 2, 'Spelling Correction': 87, 'Chinese Spelling Error Correction': 2, 'speaker-diarization': 6, 'Speaker Diarization': 6, 'DNA analysis': 1, 'Bangla Spelling Error Correction': 1, 'Mistake Detection': 1, 'Table-to-Text Generation': 34, 'Table-based Fact Verification': 6, 'Visual Dialog': 13, 'Glyph Image Generation': 1, 'Sentence Summarization': 12, 'Policy Gradient Methods': 12, 'Headline Generation': 45, 'Text Compression': 4, 'Wikipedia Summarization': 1, 'Visual Storytelling': 59, 'Dense Video Captioning': 2, 'Sequence-to-sequence Language Modeling': 3, 'Text Infilling': 24, 'News Generation': 10, 'Voice Cloning': 3, 'Distractor Generation': 15, 'Question-Answer-Generation': 8, 'Code Documentation Generation': 6, 'Source Code Summarization': 6, 'Method name prediction': 2, 'Variable misuse': 2, 'API Sequence Recommendation': 1, 'Code Comment Generation': 1, 'Contextual Embedding for Source Code': 2, 'Git Commit Message Generation': 1, 'Program Synthesis': 15, 'Concept-To-Text Generation': 6, 'Paper generation': 8, 'Paper generation (abstract-to-conclusion)': 1, 'Paper generation (Conclusion-to-title)': 1, 'Paper generation (Title-to-abstract)': 1, 'Sonnet Generation': 3, 'Isomorphism Testing': 2, 'Multi-label Image Recognition with Partial Labels': 1, 'Band Gap': 2, 'Formation Energy': 2, 'Heterogeneous Node Classification': 3, 'Length-of-Stay prediction': 3, 'Predicting Patient Outcomes': 1, '3d scene graph generation': 1, 'Information Cascade Popularity Prediction': 1, 'Inductive Relation Prediction': 3, 'Job Shop Scheduling': 4, 'Event data classification': 1, 'Acoustic Scene Classification': 2, 'Total Magnetization': 1, 'Steiner Tree Problem': 1, 'Spatio-Temporal Forecasting': 1, 'Link Sign Prediction': 3, '3D Molecule Generation': 2, 'Micro Expression Recognition': 5, 'Time Series Anomaly Detection': 2, 'Dynamic Community Detection': 1, 'Protein Folding': 3, 'Structural Node Embedding': 2, 'Fault localization': 4, 'Columns Property Annotation': 1, 'Column Type Annotation': 1, 'Supervised dimensionality reduction': 1, 'Misconceptions': 4, 'Learning Word Embeddings': 59, 'Lexical Entailment': 4, 'Multilingual Word Embeddings': 21, 'Multilingual text classification': 12, 'Personality Alignment': 1, 'Emotional Speech Synthesis': 3, 'Expressive Speech Synthesis': 1, 'Learning Network Representations': 6, 'Sequential Image Classification': 4, 'Sparse Learning': 2, 'Constituency Grammar Induction': 1, 'Zero-Shot Action Recognition': 7, '2D Pose Estimation': 4, 'Automated Theorem Proving': 7, 'Code Completion': 8, 'Program Repair': 6, 'Few-Shot Audio Classification': 1, 'Phrase Tagging': 1, 'Few-Shot Object Detection': 19, 'Unsupervised Reinforcement Learning': 5, 'Gesture Recognition': 14, 'Multimodal Reasoning': 4, 'Person Retrieval': 13, 'Music Transcription': 3, 'Passage Re-Ranking': 8, 'Video to Text Retrieval': 4, 'Point Cloud Retrieval': 1, 'Text to Video Retrieval': 14, 'Video-Text Retrieval': 16, 'Zero-Shot Video Retrieval': 14, 'Short-Text Conversation': 3, 'Dense Captioning': 2, 'Image Retrieval with Multi-Modal Query': 4, 'Fine-Grained Visual Recognition': 3, 'Action Localization': 149, 'Landmark Recognition': 3, 'Instance Search': 1, 'Image Matching': 2, '3D Point Cloud Linear Classification': 2, 'Point cloud reconstruction': 3, 'Unsupervised 3D Point Cloud Linear Evaluation': 1, '3D Shape Classification': 3, '3D Shape Recognition': 2, 'Natural Language Queries': 29, 'Connectivity Estimation': 2, 'Audio Fingerprint': 1, 'Composed Image Retrieval (CoIR)': 2, 'Music Classification': 6, 'Sketch-Based Image Retrieval': 1, 'Retrosynthesis': 3, 'Text based Person Retrieval': 9, 'Text-based Person Retrieval': 7, 'Video Grounding': 7, 'Zero-shot Composed Person Retrieval': 1, 'Music Captioning': 2, 'Text-to-Music Generation': 5, '3D Open-Vocabulary Instance Segmentation': 3, 'Zero-shot 3D classification': 2, 'Zero-Shot Transfer 3D Point Cloud Classification': 3, 'Text-To-SQL': 61, 'Veracity Classification': 1, 'Trajectory Forecasting': 5, '3D Classification': 3, 'Behavioural cloning': 2, '3D Object Retrieval': 1, 'Partially Relevant Video Retrieval': 1, 'StrategyQA': 7, 'Hyperspectral image analysis': 2, 'Deep Hashing': 134, 'Multi-Label Image Retrieval': 3, 'Sketch Recognition': 1, 'Face Image Retrieval': 2, 'Semantic Image Similarity': 1, 'Texture Image Retrieval': 1, 'Large-Scale Person Re-Identification': 4, 'Pose Retrieval': 1, '3D Action Recognition': 14, 'Heart rate estimation': 3, 'SpO2 estimation': 1, 'Pathfinder': 2, 'Pathfinder-X': 1, 'Pose Transfer': 4, 'Table Retrieval': 16, 'Table Search': 3, 'Named Entity Recognition In Vietnamese': 3, 'Pretrained Multilingual Language Models': 3, 'Propaganda detection': 9, 'Punctuation Restoration': 5, 'Document Translation': 2, 'Multiple Sequence Alignment': 1, 'Protein Language Model': 27, 'Protein Secondary Structure Prediction': 2, 'Paraphrase Identification within Bi-Encoder': 1, 'Semantic Textual Similarity within Bi-Encoder': 1, 'Fundus to Angiography Generation': 2, 'Motion Compensation': 6, 'Sign Language Translation': 11, 'Speech-to-Speech Translation': 13, 'Code Translation': 7, 'AMR Parsing': 8, 'text-to-speech translation': 1, 'Gloss-free Sign Language Translation': 2, 'Camera Localization': 1, 'Cross-corpus': 15, 'Motion Forecasting': 12, 'Task and Motion Planning': 1, 'Traffic Object Detection': 6, 'Spectral Reconstruction': 6, 'Simultaneous Speech-to-Text Translation': 2, 'Robot Manipulation': 16, 'Seizure prediction': 1, 'Sign Language Production': 1, 'Stereotypical Bias Analysis': 1, 'PointGoal Navigation': 3, 'Procedural Text Understanding': 1, 'Embodied Question Answering': 2, '3D dense captioning': 2, 'Scene-Aware Dialogue': 2, 'Vision-Language Navigation': 3, 'Prepositional Phrase Attachment': 1, 'Attribute Value Extraction': 3, 'Chart Question Answering': 7, 'Text Style Transfer': 8, 'Graph Question Answering': 8, 'Dialogue Management': 6, 'Dialogue State Tracking': 25, 'Audio Question Answering': 4, 'Brain Decoding': 5, 'Protein Interface Prediction': 1, 'Traffic Sign Detection': 2, 'Age Estimation': 18, 'Model Discovery': 2, 'Variable Selection': 4, 'PAC learning': 3, 'Program induction': 5, 'Multi-Armed Bandits': 9, 'Catalytic activity prediction': 1, 'Chemical-Disease Interaction Extraction': 1, 'Chemical Entity Recognition': 1, 'Chemical-Protein Interaction Extraction': 1, 'Description-guided molecule generation': 1, 'Domain/Motif Prediction': 1, 'Forward reaction prediction': 1, 'Functional Description Generation': 1, 'Molecular description generation': 1, 'Reagent Prediction': 1, 'True or False Question Answering': 1, 'Mathematical Proofs': 4, 'Few-Shot Imitation Learning': 3, 'Distributed Optimization': 3, 'Multi-Agent Path Finding': 2, 'Partially Labeled Datasets': 4, 'Foggy Scene Segmentation': 1, 'Thompson Sampling': 6, 'Semantic SLAM': 1, 'Few-shot NER': 24, 'Open-Domain Dialog': 4, 'Extreme Multi-Label Classification': 7, 'Question Rewriting': 12, 'Geographic Question Answering': 1, 'abstractive question answering': 2, 'Conversational Question Answering': 80, 'Compiler Optimization': 1, 'Multi-modal Knowledge Graph': 1, 'Dialogue Act Classification': 6, 'Audio Signal Processing': 1, 'Question Selection': 3, 'Knowledge Base Question Answering': 57, 'Graph Ranking': 2, 'Fact Selection': 1, 'Semantic Text Matching': 4, 'Question Quality Assessment': 1, 'Anachronisms': 1, 'Analogical Similarity': 1, 'Analytic Entailment': 1, 'Crash Blossom': 1, 'Crass AI': 1, 'Dark Humor Detection': 1, 'Discourse Marker Prediction': 1, 'Empirical Judgments': 1, 'English Proverbs': 1, 'Entailed Polarity': 1, 'Epistemic Reasoning': 1, 'Evaluating Information Essentiality': 1, 'Fantasy Reasoning': 1, 'Figure Of Speech Detection': 1, 'GRE Reading Comprehension': 1, 'Human Organs Senses Multiple Choice': 1, 'Identify Odd Metapor': 1, 'Implicatures': 1, 'Irony Identification': 1, 'Known Unknowns': 1, 'LAMBADA': 3, 'Logical Args': 1, 'Logical Fallacy Detection': 1, 'Logic Grid Puzzle': 1, 'Mathematical Induction': 2, 'Metaphor Boolean': 1, 'Moral Permissibility': 1, 'Movie Dialog Same Or Different': 1, 'Nonsense Words Grammar': 1, 'Phrase Relatedness': 1, 'Physical Intuition': 1, 'Physics MC': 1, 'Presuppositions As NLI': 1, 'Sentence Ambiguity': 1, 'Similarities Abstraction': 1, 'Timedial': 1, 'Understanding Fables': 1, 'Winowhy': 1, 'Fill Mask': 1, 'TGIF-Frame': 8, 'Video-based Generative Performance Benchmarking': 5, 'Video-based Generative Performance Benchmarking (Consistency)': 5, 'Video-based Generative Performance Benchmarking (Contextual Understanding)': 5, 'Video-based Generative Performance Benchmarking (Correctness of Information)': 5, 'Video-based Generative Performance Benchmarking (Detail Orientation))': 5, 'Video-based Generative Performance Benchmarking (Temporal Understanding)': 5, 'zero-shot long video breakpoint-model question answering': 1, 'zero-shot long video breakpoint-mode question answering': 1, 'zero-shot long video global-model question answering': 1, 'zero-shot long video global-mode question answering': 1, 'zero-shot long video question answering': 1, 'video narration captioning': 1, 'Cross-Lingual Question Answering': 11, 'Generative Question Answering': 24, 'Mathematical Question Answering': 4, 'Query-Based Extractive Summarization': 2, 'Split and Rephrase': 2, 'Text-Dependent Speaker Verification': 1, 'Medical Report Generation': 6, 'Transliteration': 137, 'Hope Speech Detection': 3, 'Cross-Lingual Entity Linking': 2, 'Code Repair': 1, 'Automatic Post-Editing': 60, 'Low-Resource Neural Machine Translation': 52, 'Arrhythmia Detection': 1, 'Electrocardiography (ECG)': 2, 'End-To-End Dialogue Modelling': 1, 'Action Recognition In Videos': 28, 'Environmental Sound Classification': 1, 'Sound Classification': 4, '3D Hand Pose Estimation': 3, 'Text-Independent Speaker Verification': 1, 'Sound Event Localization and Detection': 4, '6D Pose Estimation': 7, '6D Pose Estimation using RGB': 7, '6D Pose Estimation using RGBD': 2, 'Audio Source Separation': 4, 'Voice Anti-spoofing': 1, 'ECG Classification': 1, '2D Object Detection': 15, 'Small Object Detection': 9, 'Mental Stress Detection': 1, 'Physiological Computing': 2, 'License Plate Detection': 2, 'License Plate Recognition': 3, 'Amodal Instance Segmentation': 1, 'Object Categorization': 6, 'Classification Of Variable Stars': 1, 'Rotated MNIST': 1, 'Graph Outlier Detection': 1, 'Amodal Tracking': 1, 'Automatic Lyrics Transcription': 1, 'Fault Detection': 4, 'Semi-supervised Anomaly Detection': 2, 'Hand Gesture Recognition': 5, 'Hand-Gesture Recognition': 3, 'One-shot Unsupervised Domain Adaptation': 1, 'tabular-classification': 4, 'Diabetes Prediction': 1, 'Object Detection In Aerial Images': 10, 'Solar Flare Prediction': 1, 'breast density classification': 1, 'Automatic Modulation Recognition': 2, 'Photoplethysmography (PPG)': 1, 'Table Detection': 3, \"Bird's-Eye View Semantic Segmentation\": 2, 'Future prediction': 4, 'Landslide segmentation': 1, 'Clone Detection': 3, 'Fire Detection': 1, 'Road Damage Detection': 1, 'Satellite Image Classification': 1, 'Malware Family Detection': 1, 'Instrument Recognition': 6, 'Semi Supervised Learning for Image Captioning': 1, 'Unsupervised Human Pose Estimation': 2, 'Unsupervised Keypoint Estimation': 1, 'Driver Authentication': 1, 'Driver Identification': 2, 'Activity Recognition In Videos': 5, 'No real Data Binarization': 1, 'Synthetic Data Binarization': 1, 'Image ClassiData Augmentationfication': 1, 'Gender Classification': 9, 'Social Navigation': 2, 'Fish Detection': 2, 'Abusive Language': 19, 'Zero-Shot Text-to-Image Generation': 10, 'Concept Alignment': 13, 'Multi Class Text Classification': 5, 'Binary text classification': 2, 'Multi-Modal Document Classification': 2, 'Information Extraction': 2, 'Inductive logic programming': 2, 'Image-text Classification': 5, 'Multilabel Text Classification': 3, 'Few-shot HTC': 1, 'Multilingual Image-Text Classification': 1, 'Vision and Language Navigation': 2, 'Product Categorization': 4, 'Zero-shot Cross-Lingual Document Classification': 1, 'Medical Code Prediction': 4, 'Medical Codes Prediction': 1, 'Cross-Domain Document Classification': 1, 'Document Text Classification': 2, 'Job classification': 2, 'Taxonomy Expansion': 2, 'cross-domain few-shot learning': 17, 'Component Classification': 1, 'Dynamic Topic Modeling': 1, 'Spectral Graph Clustering': 1, 'ListOps': 2, 'Suggestion mining': 3, 'Music Modeling': 1, 'Toxic Spans Detection': 1, 'Molecular System Prediction': 1, 'Dialog Act Classification': 3, 'Native Language Identification': 3, 'Emotion Classification': 262, 'Multimodal Emotion Recognition': 73, 'Speech Denoising': 7, 'Melody Extraction': 2, 'Visu': 2, 'Seizure Detection': 2, 'Arousal Estimation': 8, 'Dominance Estimation': 1, 'Valence Estimation': 4, 'Facial Emotion Recognition': 24, 'Emotion Cause Extraction': 3, 'Music Emotion Recognition': 6, 'Action Unit Detection': 8, 'Facial Action Unit Detection': 6, 'Emotion-Cause Pair Extraction': 5, 'Video Emotion Recognition': 8, 'EEG Emotion Recognition': 12, 'Blood pressure estimation': 1, 'Multi-Label Classification Of Biomedical Texts': 3, 'Dynamic Reconstruction': 2, 'News Annotation': 2, 'Aggression Identification': 3, 'Handwritten Digit Recognition': 1, 'Semi-supervised Audio Classification': 1, 'Toxic Comment Classification': 18, 'Classification of toxic, engaging, fact-claiming comments': 1, 'Engaging Comment Classification': 1, 'Fact-Claiming Comment Classification': 1, 'Complaint Comment Classification': 1, 'Constructive Comment Classification': 2, 'Vietnamese Datasets': 4, 'Coherence Evaluation': 16, 'Nested Mention Recognition': 2, 'Nested Named Entity Recognition': 23, 'Joint NER and Classification': 1, 'Multilingual Named Entity Recognition': 7, 'Chemical Indexing': 1, 'Weakly-Supervised Named Entity Recognition': 3, 'Multi-modal Named Entity Recognition': 1, 'Cross-Lingual NER': 15, 'Panoptic Scene Graph Generation': 1, 'AudioCaps': 3, 'Audio to Text Retrieval': 1, 'Audio-Visual Question Answering (AVQA)': 4, 'Text to Audio Retrieval': 1, 'Zero-Shot Environment Sound Classification': 2, 'Aesthetics Quality Assessment': 2, 'Video Stabilization': 1, 'TGIF-Action': 4, 'TGIF-Transition': 4, 'Video Enhancement': 10, 'Spatio-Temporal Action Localization': 15, 'Add - PO': 1, 'Add - PQ': 1, 'Counterfactual Planning': 1, 'Remove - PO': 1, 'Remove - PQ': 1, 'Replace - PO': 1, 'Replace - PQ': 1, 'Network Interpretation': 1, 'Blind Image Quality Assessment': 3, 'visual instruction following': 2, 'Explanatory Visual Question Answering': 1, 'Acoustic Question Answering': 1, 'Memex Question Answering': 1, 'Object Proposal Generation': 4, 'Audio-visual Question Answering': 3, 'Formal Logic': 2, 'Meme Captioning': 1, 'Image Comprehension': 1, 'One-Shot Instance Segmentation': 1, 'One-Shot Object Detection': 2, 'road scene understanding': 2, 'Cross-lingual zero-shot dependency parsing': 1, 'Long-tail learning with class descriptors': 1, 'Generalized Few-Shot Classification': 1, 'Unsupervised Few-Shot Image Classification': 4, 'Unsupervised Few-Shot Learning': 2, 'Training-free 3D Part Segmentation': 1, 'Training-free 3D Point Cloud Classification': 1, 'Common Sense Reasoning (Zero-Shot)': 2, 'Natural Language Inference (Zero-Shot)': 2, 'Sequential skip prediction': 1, 'Few-Shot action recognition': 7, 'Few Shot Action Recognition': 9, 'Few-Shot Point Cloud Classification': 2, 'Few-shot 3D Point Cloud Semantic Segmentation': 1, 'Ischemic Stroke Lesion Segmentation': 14, 'Multi class one-shot image synthesis': 1, 'Single class few-shot image synthesis': 1, 'Molecular Docking': 2, 'Spoken language identification': 2, 'Surgical phase recognition': 2, 'Anomaly Classification': 2, 'High School Physics': 1, 'Cover song identification': 1, 'Music Tagging': 2, 'Table annotation': 2, 'Online Ranker Evaluation': 1, 'Timex normalization': 3, 'Music Auto-Tagging': 2, 'Temporal Tagging': 1, 'Chord Recognition': 2, 'Music Genre Classification': 1, 'Chat-based Image Retrieval': 1, 'Toponym Resolution': 1, 'Cross-Modal Information Retrieval': 1, 'FAD': 1, 'Document-level Event Extraction': 1, '3D Place Recognition': 1, 'Loop Closure Detection': 1, 'Exposure Fairness': 1, 'Multi-Choice MRC': 5, 'Multi-Hop Reading Comprehension': 5, 'Multilingual Machine Comprehension in English Hindi': 1, 'Chinese Reading Comprehension': 2, 'Learning to Execute': 1, 'Video Relationship': 1, 'Multi-domain Dialogue State Tracking': 3, 'Probing Language Models': 1, 'ContextNER': 1, 'Span-Extraction MRC': 3, 'Label Error Detection': 1, 'Vietnamese Machine Reading Comprehension': 1, 'Lexical Complexity Prediction': 1, 'WNLI': 2, 'Unsupervised Sentence Summarization': 3, 'Self-supervised Video Retrieval': 1, 'Phrase Vector Embedding': 1, 'STS-B': 2, 'trustable and focussed LLM generated content': 1, 'Vocabulary-free Image Classification': 1, 'Weakly-supervised panoptic segmentation': 1, 'Multi-Instance Retrieval': 1, 'Conversational Response Generation': 3, 'Sentence ReWriting': 4, 'Meeting Summarization': 6, 'Turning Point Identification': 1, 'Automated Writing Evaluation': 1, 'Unsupervised Sentence Compression': 1, 'Transfer Reinforcement Learning': 1, 'Negation Detection': 2, 'Summarization': 1, 'Multimodal Abstractive Text Summarization': 1, 'Models Alignment': 1, 'Unsupervised Text Summarization': 1, 'Zero-shot Text to Audio Retrieval': 1, 'Generative Visual Question Answering': 2, 'Music Question Answering': 2, 'Bug fixing': 1, 'Vulnerability Detection': 2, 'Robot Task Planning': 2, 'Formality Style Transfer': 4, 'Zero-shot Image Retrieval': 1, 'Cell Entity Annotation': 1, 'Single-cell modeling': 1, 'Multi-Objective Reinforcement Learning': 4, 'Target Speaker Extraction': 1, 'NetHack': 1, 'Missing Elements': 1, 'Misogynistic Aggression Identification': 1, 'Hate Speech Normalization': 1, 'Meme Classification': 2, 'Few-Shot NLI': 2, 'Text-to-video search': 2, 'Zero-Shot Cross-Lingual Image-to-Text Retrieval': 1, 'Zero-Shot Cross-Lingual Text-to-Image Retrieval': 1, 'Zero-Shot Cross-Lingual Visual Natural Language Inference': 1, 'Zero-Shot Cross-Lingual Visual Question Answering': 1, 'Zero-Shot Cross-Lingual Visual Reasoning': 1, 'Cross-lingual Fact-checking': 1, 'Zero-shot Cross-lingual Fact-checking': 1, 'Zero-Shot Machine Translation': 1, 'Cross-Lingual ASR': 2, 'Semantic Dependency Parsing': 4, 'Complex Word Identification': 1, 'Few-shot Age Estimation': 1, 'Historical Color Image Dating': 1, 'Real-World Adversarial Attack': 3, 'Card Games': 5, 'Electrical Engineering': 1, 'Physics-informed machine learning': 1, 'Speech Intent Classification': 1, 'Log Parsing': 1, 'Text-to-Code Generation': 1, 'controllable image captioning': 1, 'Emotion Recognition in Context': 2, 'Heart Rate Variability': 1, 'Spoken Command Recognition': 1, 'Sparse Representation-based Classification': 1, '3D Facial Expression Recognition': 1, 'Dynamic Facial Expression Recognition': 3, 'Age Classification': 1, 'A-VB High': 1, 'Chinese Sentiment Analysis': 1, 'Emotional Dialogue Acts': 2, 'Personality Recognition in Conversation': 1, 'Personality Trait Recognition': 1, 'Personalized and Emotional Conversation': 1, 'Spatial Interpolation': 1, 'Body Detection': 1, 'Audio Emotion Recognition': 1, 'Cloze (multi-choices) (Few-Shot)': 1, 'Cloze (multi-choices) (One-Shot)': 1, 'Cloze (multi-choices) (Zero-Shot)': 1, 'Common Sense Reasoning (Few-Shot)': 1, 'Common Sense Reasoning (One-Shot)': 1, 'Natural Language Inference (Few-Shot)': 1, 'Natural Language Inference (One-Shot)': 1, 'Reading Comprehension (Few-Shot)': 1, 'Reading Comprehension (One-Shot)': 1, 'Reading Comprehension (Zero-Shot)': 1, 'Physical Commonsense Reasoning': 1, 'Exception type': 1, 'Function-docstring mismatch': 1, 'Swapped operands': 1, 'Wrong binary operator': 1, 'Game of Chess': 2, 'Short-term Object Interaction Anticipation': 1, 'text-based games': 1, 'Natural Language Visual Grounding': 1, 'Prosody Prediction': 1, 'Zero-Shot Intent Classification': 1, 'Sketch-to-text Generation': 1, 'few-shot-ner': 12, 'Miscellaneous': 7, 'FG-1-PG-1': 1, 'SENTER': 1, 'Scholarly Named Entity Recognition': 1, 'Reading Order Detection': 1, 'Jurisprudence': 1, 'Human Part Segmentation': 5, 'Semantic Frame Parsing': 1, 'UCCA Parsing': 4, 'DRS Parsing': 2, 'SQL Parsing': 9, 'Geometry Problem Solving': 1, 'Page Stream Segmentation': 1, 'Multimodal Activity Recognition': 7, 'Temporal Action Proposal Generation': 7, 'Actin Detection': 1, 'RF-based Pose Estimation': 1, 'Camera shot boundary detection': 1, 'Fine-grained Action Recognition': 4, 'Skills Assessment': 2, 'Weakly Supervised Action Localization': 21, 'Weakly-supervised Temporal Action Localization': 44, 'Weakly Supervised Temporal Action Localization': 44, 'Egocentric Activity Recognition': 5, 'One-Shot 3D Action Recognition': 1, 'Multi-modal Classification': 3, 'Weakly-Supervised Action Recognition': 3, 'Moment Queries': 4, 'Online Action Detection': 2, 'Action Analysis': 3, 'Action Spotting': 1, 'Motion Magnification': 2, 'Electromyography (EMG)': 1, 'Zero-Shot Action Detection': 1, 'Wearable Activity Recognition': 2, 'Open Set Action Recognition': 1, 'Fine-Grained Action Detection': 4, 'Human Interaction Recognition': 1, 'Sports Analytics': 3, 'Unsupervised Skeleton Based Action Recognition': 1, 'Few Shot Temporal Action Localization': 2, 'Semi-Supervised Action Detection': 1, 'Action Recognition on HMDB-51': 1, 'Gait Recognition': 4, 'Multi-Object Tracking and Segmentation': 2, 'Action Recognition In Still Images': 1, 'Video Similarity': 2, 'Violence and Weaponized Violence Detection': 1, 'Multi-Person Pose Estimation and Tracking': 1, 'Atomic action recognition': 1, 'Composite action recognition': 1, 'Action Generation': 3, 'Human action generation': 2, 'Human Mesh Recovery': 3, 'Human Instance Segmentation': 2, 'Temporal Sentence Grounding': 2, 'Unfairness Detection': 1, 'Blood Cell Detection': 3, 'Electron Microscopy Image Segmentation': 1, 'Semi-Supervised Instance Segmentation': 2, 'Histopathological Image Classification': 3, 'Nuclear Segmentation': 3, 'Salt-And-Pepper Noise Removal': 1, 'Color Image Denoising': 10, 'Image Deblocking': 3, 'JPEG Artifact Correction': 6, 'Jpeg Compression Artifact Reduction': 2, 'Grayscale Image Denoising': 3, 'Audio Tagging': 2, 'Tone Mapping': 1, 'Sar Image Despeckling': 3, 'Image Deconvolution': 2, 'Image Compression Artifact Reduction': 1, 'Non-Intrusive Load Monitoring': 1, 'Demosaicking': 11, 'Video Reconstruction': 3, 'Hyperspectral Image Denoising': 10, 'Medical Image Denoising': 1, 'Time Series Denoising': 1, 'Lossy-Compression Artifact Reduction': 1, 'Exponential degradation': 1, 'Video Restoration': 3, 'Video Panoptic Segmentation': 1, 'Noise Level Prediction': 1, '3D Inpainting': 1, 'Unsupervised Keypoints': 2, 'Electron Tomography': 1, 'Quantum Circuit Generation': 1, 'Generalized Zero-Shot Object Detection': 1, 'Generalized Zero-Shot Object Detection on MS-COCO': 1, 'Point Cloud Super Resolution': 3, 'Indoor Scene Reconstruction': 2, 'Bandwidth Extension': 1, '4D reconstruction': 1, 'Stochastic Human Motion Prediction': 1, 'Human Dynamics': 2, 'Pedestrian Trajectory Prediction': 4, 'Document Enhancement': 1, 'Depth And Camera Motion': 6, 'Stereo Depth Estimation': 17, 'Stereo-LiDAR Fusion': 2, 'Point Clouds': 3, '3D Object Super-Resolution': 1, '3D Geometry Perception': 1, '3D Object Detection From Stereo Images': 2, '3D Depth Estimation': 2, 'Monocular Visual Odometry': 2, 'Robust Camera Only 3D Object Detection': 4, 'Indoor Monocular Depth Estimation': 2, 'Scene Flow Estimation': 3, 'Monocular 3D Object Localization': 1, 'Plane Instance Segmentation': 1, 'Trajectory Planning': 5, 'Egocentric Pose Estimation': 1, '3D Semantic Scene Completion from a single RGB image': 3, '3D Single Object Tracking': 4, 'Point Cloud Completion': 5, 'Transparent Object Depth Estimation': 1, 'Multi-object discovery': 1, 'Transparent Object Detection': 1, 'Automatic Machine Learning Model Selection': 1, 'Breast Mass Segmentation In Whole Mammograms': 1, 'Probabilistic Deep Learning': 1, 'Blood Cell Count': 2, 'Classification with Costly Features': 1, 'Medical Procedure': 3, 'Respiratory Failure': 1, 'Disease Trajectory Forecasting': 1, 'MonkeyPox Diagnosis': 1, 'Clinical Information Retreival': 1, 'Clinical Language Translation': 1, 'Professional Medicine': 1, 'Coronary Artery Segmentation': 2, 'dialogue summary': 1, 'Side Channel Analysis': 1, 'Automatic Cell Counting': 1, 'Multi-Source Unsupervised Domain Adaptation': 7, 'Unsupervised Person Re-Identification': 33, 'Drivable Area Detection': 6, 'Partial Domain Adaptation': 5, 'Multi-Human Parsing': 4, 'Image Quality Estimation': 2, 'Long-tailed Object Detection': 2, 'Medical Image Enhancement': 1, 'Infrared image super-resolution': 1, 'Drug Response Prediction': 1, 'Face Presentation Attack Detection': 8, 'Event-based vision': 5, 'Birds Eye View Object Detection': 3, 'Robust 3D Object Detection': 4, 'Robust 3D Semantic Segmentation': 7, '3D Object Detection From Monocular Images': 1, 'Vehicle Pose Estimation': 3, '3D Multi-Object Tracking': 5, 'Traffic Accident Detection': 1, 'Safe Reinforcement Learning': 14, 'Smart Grid Prediction': 1, 'Driver Attention Monitoring': 1, '': 3, 'HD semantic map learning': 1, '3D Lane Detection': 5, 'CARLA longest6': 2, 'Radar Object Detection': 1, '3D Unsupervised Domain Adaptation': 1, 'Online unsupervised domain adaptation': 1, 'Box-supervised Instance Segmentation': 2, 'Oriented Object Detection': 11, 'CARLA MAP Leaderboard': 1, '3D Object Tracking': 2, 'Trajectory Clustering': 1, 'Unsupervised Vehicle Re-Identification': 2, 'Prediction Of Occupancy Grid Maps': 1, '3D Facial Landmark Localization': 1, 'Valene Estimation': 1, 'Causal Emotion Entailment': 2, 'Sentence': 74, 'Commonsense Knowledge Base Construction': 1, 'Vietnamese Aspect-Based Sentiment Analysis': 1, 'Disguised Face Verification': 1, 'Face Recognition (Closed-Set)': 1, 'Surveillance-to-Booking': 2, 'Surveillance-to-Single': 2, 'Surveillance-to-Surveillance': 2, 'Robust Face Alignment': 1, 'Face Image Quality': 16, 'Face Quality Assessement': 3, 'Age and Gender Estimation': 1, 'Unsupervised face recognition': 2, 'Template Matching': 1, 'Hierarchical Text Classification of Blurbs (GermEval 2019)': 1, 'TAR': 4, 'Synthetic Face Recognition': 3, 'Face Morphing Attack Detection': 7, 'Face Image Quality Assessment': 11, 'Classification Consistency': 1, 'Crime Prediction': 1, 'Annotated Code Search': 1, 'Visual Text Correction': 1, 'HDR Reconstruction': 1, 'Event-Based Video Reconstruction': 1, 'Spectral Super-Resolution': 3, 'Brain Lesion Segmentation From Mri': 5, 'Unbalanced Segmentation': 1, 'Automatic Liver And Tumor Segmentation': 2, 'Acute Stroke Lesion Segmentation': 2, 'Outcome Prediction In Multimodal Mri': 1, 'Melanoma Diagnosis': 6, 'Organ Detection': 1, 'COVID-19 Image Segmentation': 2, 'Local Color Enhancement': 1, 'User Simulation': 2, 'Holdout Set': 1, 'Prostate Zones Segmentation': 1, 'Myocardium Segmentation': 1, 'Camouflage Segmentation': 1, 'Real-time instance measurement': 1, 'Stenosis Segmentation': 1, 'Pseudo Label Filtering': 1, 'Fine-Grained Visual Categorization': 2, \"Rubik's Cube\": 2, 'continual few-shot learning': 1, 'Traffic Classification': 1, 'Unbiased Scene Graph Generation': 1, 'Univariate Time Series Forecasting': 1, 'Photo geolocation estimation': 1, 'Evolving Domain Generalization': 1, 'Novel Object Detection': 4, 'Indoor Localization': 3, 'Epilepsy Prediction': 1, 'Second-order methods': 2, 'Value prediction': 2, 'Safe Exploration': 4, 'Hardware Aware Neural Architecture Search': 1, 'One-stage Anchor-free Oriented Object Detection': 3, 'Dense Object Detection': 3, 'Surgical tool detection': 2, 'Object Segmentation': 1, 'Object Detection In Indoor Scenes': 1, 'Video Object Tracking': 2, 'RGB-D Salient Object Detection': 2, 'Online Multi-Object Tracking': 3, 'Co-Salient Object Detection': 3, 'Long-tail Video Object Segmentation': 1, 'Open-World Instance Segmentation': 1, 'Camouflaged Object Segmentation with a Single Task-generic Prompt': 1, 'Saliency Ranking': 1, 'RGBD Semantic Segmentation': 2, 'Few Shot Open Set Object Detection': 1, 'Multiview Detection': 1, 'Total Energy': 1, 'Video Salient Object Detection': 1, 'Zero-Shot Counting': 1, 'Scene Change Detection': 1, 'Speaker-Specific Lip to Speech Synthesis': 1, 'Multiple People Tracking': 1, 'Real-Time Strategy Games': 3, 'Skills Evaluation': 1, 'Domain Adaptive Person Re-Identification': 4, 'Clothes Changing Person Re-Identification': 3, 'Unsupervised Clothes Changing Person Re-Identification': 1, 'Unsupervised Long Term Person Re-Identification': 1, 'Image Steganography': 3, 'Part-based Representation Learning': 1, 'Federated Lifelong Person ReID': 1, 'Cross-Modality Person Re-identification': 1, 'SMAC+': 6, 'Game of Go': 2, 'Game of Shogi': 1, \"Montezuma's Revenge\": 3, 'Multi-Goal Reinforcement Learning': 2, 'Portfolio Optimization': 2, 'Car Racing': 3, 'Distributional Reinforcement Learning': 5, 'SMAC': 3, 'DQN Replay Dataset': 1, 'Action Triplet Recognition': 1, 'FPS Games': 3, 'Game of Doom': 2, 'Game of Poker': 2, 'Molecular Graph Generation': 1, 'Atari Games 100k': 2, 'Unsupervised Video Summarization': 1, 'Procgen Hard (100M)': 1, 'Partially Observable Reinforcement Learning': 1, 'Control with Prametrised Actions': 1, 'Dialog Learning': 1, 'Early Classification': 1, 'Trajectory Modeling': 1, 'Acrobot': 1, 'energy management': 1, 'Single-object discovery': 1, 'Weakly-Supervised Object Segmentation': 1, '3D Absolute Human Pose Estimation': 1, 'Portrait Segmentation': 1, '4D Spatio Temporal Semantic Segmentation': 1, 'Unsupervised MNIST': 1, 'Unsupervised Semantic Segmentation with Language-image Pre-training': 2, 'Zero-Shot Video Object Segmentation': 1, 'Placenta Segmentation': 3, 'Crack Segmentation': 1, 'Time-Series Few-Shot Learning with Heterogeneous Channels': 1, 'Multi-Frame Super-Resolution': 5, 'Audio Super-Resolution': 3, 'satellite image super-resolution': 2, 'Stereo Image Super-Resolution': 4, '3D Surface Generation': 1, 'Point Set Upsampling': 1, 'Space-time Video Super-resolution': 3, 'Burst Image Super-Resolution': 3, 'Analog Video Restoration': 1, 'Compressed Image Super-resolution': 1, 'Video Deinterlacing': 1, 'Image Imputation': 1, 'Reference-based Super-Resolution': 2, 'Hyperspectral Image Super-Resolution': 4, '3D human pose and shape estimation': 1, 'Burst Image Reconstruction': 1, 'Motion Interpolation': 1, 'Bokeh Effect Rendering': 1, 'Nonhomogeneous Image Dehazing': 1, 'Optic Cup Segmentation': 2, 'Histopathological Segmentation': 1}\n"]},{"output_type":"execute_result","data":{"text/plain":["2139"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#dropping rows with only rare tasks\n","tasks_list = df.tasks.to_list()\n","revised_task_list = []\n","indices_to_drop = []\n","\n","for idx, tasks in enumerate(tasks_list):\n","  task_list = eval(tasks)\n","  revised_tasks = []\n","\n","  for task in task_list:\n","    if task not in rare_tasks:\n","      revised_tasks.append(task)\n","\n","  if len(revised_tasks) == 0:\n","    indices_to_drop.append(idx)\n","  else:\n","    revised_task_list.append(revised_tasks)\n","\n","df = df.drop(indices_to_drop).reset_index(drop=True)\n","df['revised_tasks'] = revised_task_list\n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZ_J5T23yLQM","executionInfo":{"status":"ok","timestamp":1706449829004,"user_tz":-360,"elapsed":2882,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"e9583045-b7a8-4328-e777-4fcd8b3835a5"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26628, 5)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["revised_tasks_list = df.revised_tasks.to_list()\n","revised_task_count = {}\n","for tasks in revised_tasks_list:\n","  task_list = tasks\n","  for task in task_list:\n","    if task in revised_task_count.keys():\n","      revised_task_count[task] += 1\n","    else:\n","       revised_task_count[task]= 1\n","print(f\"Number of Tasks: {len(revised_task_count)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxYdpV_GydOR","executionInfo":{"status":"ok","timestamp":1706449831896,"user_tz":-360,"elapsed":632,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"d2b0853e-ac25-473f-df06-0cfb5310f9dc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Tasks: 258\n"]}]},{"cell_type":"code","source":["encode_task_types = { key: idx for idx, (key, value) in enumerate(revised_task_count.items())}\n","with open(\"task_types_encoded.json\", \"w\") as fp:\n","  json.dump(encode_task_types, fp)\n","\n","# We need this because for multilabel classification all genres have possibility to be present in the predictions\n","categorical_task_list = []\n","revised_tasks_list = df.revised_tasks.to_list()\n","\n","for revised_tasks in revised_tasks_list:\n","  categorical_list = [0] * len(encode_task_types)\n","  for task in revised_tasks:\n","    task_type_index = encode_task_types[task]\n","    categorical_list[task_type_index] = 1\n","  categorical_task_list.append(categorical_list)\n","\n","df['task_cat_list']=categorical_task_list\n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emcfDKz2yfQg","executionInfo":{"status":"ok","timestamp":1706449852258,"user_tz":-360,"elapsed":1189,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"c1d2911c-08f2-4296-c75a-f7dca9c87033"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26628, 6)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["labels = list(encode_task_types.keys())\n","len(labels), labels[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqozxSKSymTR","executionInfo":{"status":"ok","timestamp":1706449861579,"user_tz":-360,"elapsed":722,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"f8a9e666-30d9-4f41-be12-fc4be3e23d4f"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(258,\n"," ['Active Learning',\n","  'Image Classification',\n","  'Semantic Segmentation',\n","  'object-detection',\n","  'Object Detection'])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# Data Split"],"metadata":{"id":"6UIk-nebzjp3"}},{"cell_type":"code","source":["splitter = RandomSplitter(valid_pct=0.1, seed=42)\n","train_ids, valid_ids = splitter(df)\n","len(train_ids), len(valid_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlCDrzRN0BLn","executionInfo":{"status":"ok","timestamp":1706449871524,"user_tz":-360,"elapsed":636,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"72d6072a-9510-4ef2-af07-75ec879e0140"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(23966, 2662)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["valid_df = df.loc[valid_ids]\n","valid_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ONSE-r4UEiWD","executionInfo":{"status":"ok","timestamp":1706449874389,"user_tz":-360,"elapsed":12,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"8d46e2c5-ed0a-4651-ad96-b6d4671af45c"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                              title  \\\n","13434                                    Feature-aware conditional GAN for category text generation   \n","9227                                            Network Embedding with Completely-imbalanced Labels   \n","3396   Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment   \n","12667     Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages   \n","21614                   Task-agnostic Out-of-Distribution Detection Using Kernel Density Estimation   \n","\n","                                                                                   url  \\\n","13434      https://paperswithcode.com/paper/feature-aware-conditional-gan-for-category   \n","9227   https://paperswithcode.com/paper/network-embedding-with-completely-imbalanced-1   \n","3396     https://paperswithcode.com/paper/rethinking-uncertainly-missing-and-ambiguous   \n","12667   https://paperswithcode.com/paper/revisiting-plasticity-in-visual-reinforcement   \n","21614    https://paperswithcode.com/paper/unsupervised-out-of-distribution-detection-2   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract  \\\n","13434  Category text generation receives considerable attentions since it is beneficial for various natural language processing tasks. Recently, the generative adversarial network (GAN) has attained promising performance in text generation, attributed to its adversarial training process. However, there are several issues in text GANs, including discreteness, training instability, mode collapse, lack of diversity and controllability etc. To address these issues, this paper proposes a novel GAN framework, the feature-aware conditional GAN (FA-GAN), for controllable category text generation. In FA-G...   \n","9227   Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSD...   \n","3396   As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the...   \n","12667  Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the c...   \n","21614  In the recent years, researchers proposed a number of successful methods to perform out-of-distribution (OOD) detection in deep neural networks (DNNs). So far the scope of the highly accurate methods has been limited to image level classification tasks. However, attempts for generally applicable methods beyond classification did not attain similar performance. In this paper, we address this limitation by proposing a simple yet effective task-agnostic OOD detection method. We estimate the probability density functions (pdfs) of intermediate features of a pre-trained DNN by performing kernel...   \n","\n","                                                                                                                                                                                                                                                                    tasks  \\\n","13434                                                                                                                                   ['Generative Adversarial Network', 'Multi-class Classification', 'text-classification', 'Text Classification', 'Text Generation']   \n","9227                                                                                                                                                                                                                                                ['Network Embedding']   \n","3396                                                                                                                                               ['Benchmarking', 'Entity Alignment', 'Knowledge Graph Embeddings', 'Knowledge Graphs', 'Multi-modal Entity Alignment']   \n","12667                                                                                                                                                                                                                     ['Data Augmentation', 'reinforcement-learning']   \n","21614  ['Autonomous Driving', 'Classification', 'Density Estimation', 'General Classification', 'Image Segmentation', 'Medical Diagnosis', 'Medical Image Segmentation', 'Out-of-Distribution Detection', 'Out of Distribution (OOD) Detection', 'Semantic Segmentation']   \n","\n","                                                                                                                                                revised_tasks  \\\n","13434                                                             [Generative Adversarial Network, text-classification, Text Classification, Text Generation]   \n","9227                                                                                                                                      [Network Embedding]   \n","3396                                                                                             [Benchmarking, Knowledge Graph Embeddings, Knowledge Graphs]   \n","12667                                                                                                             [Data Augmentation, reinforcement-learning]   \n","21614  [Autonomous Driving, Classification, General Classification, Image Segmentation, Medical Diagnosis, Medical Image Segmentation, Semantic Segmentation]   \n","\n","                                                                                                                                                                                                                                                                                                           task_cat_list  \n","13434  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n","9227   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n","3396   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n","12667  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n","21614  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  "],"text/html":["\n","  <div id=\"df-91a9dc2a-5b53-49c6-859b-4c5819918b71\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>abstract</th>\n","      <th>tasks</th>\n","      <th>revised_tasks</th>\n","      <th>task_cat_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13434</th>\n","      <td>Feature-aware conditional GAN for category text generation</td>\n","      <td>https://paperswithcode.com/paper/feature-aware-conditional-gan-for-category</td>\n","      <td>Category text generation receives considerable attentions since it is beneficial for various natural language processing tasks. Recently, the generative adversarial network (GAN) has attained promising performance in text generation, attributed to its adversarial training process. However, there are several issues in text GANs, including discreteness, training instability, mode collapse, lack of diversity and controllability etc. To address these issues, this paper proposes a novel GAN framework, the feature-aware conditional GAN (FA-GAN), for controllable category text generation. In FA-G...</td>\n","      <td>['Generative Adversarial Network', 'Multi-class Classification', 'text-classification', 'Text Classification', 'Text Generation']</td>\n","      <td>[Generative Adversarial Network, text-classification, Text Classification, Text Generation]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n","    </tr>\n","    <tr>\n","      <th>9227</th>\n","      <td>Network Embedding with Completely-imbalanced Labels</td>\n","      <td>https://paperswithcode.com/paper/network-embedding-with-completely-imbalanced-1</td>\n","      <td>Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSD...</td>\n","      <td>['Network Embedding']</td>\n","      <td>[Network Embedding]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n","    </tr>\n","    <tr>\n","      <th>3396</th>\n","      <td>Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment</td>\n","      <td>https://paperswithcode.com/paper/rethinking-uncertainly-missing-and-ambiguous</td>\n","      <td>As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the...</td>\n","      <td>['Benchmarking', 'Entity Alignment', 'Knowledge Graph Embeddings', 'Knowledge Graphs', 'Multi-modal Entity Alignment']</td>\n","      <td>[Benchmarking, Knowledge Graph Embeddings, Knowledge Graphs]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n","    </tr>\n","    <tr>\n","      <th>12667</th>\n","      <td>Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages</td>\n","      <td>https://paperswithcode.com/paper/revisiting-plasticity-in-visual-reinforcement</td>\n","      <td>Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the c...</td>\n","      <td>['Data Augmentation', 'reinforcement-learning']</td>\n","      <td>[Data Augmentation, reinforcement-learning]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n","    </tr>\n","    <tr>\n","      <th>21614</th>\n","      <td>Task-agnostic Out-of-Distribution Detection Using Kernel Density Estimation</td>\n","      <td>https://paperswithcode.com/paper/unsupervised-out-of-distribution-detection-2</td>\n","      <td>In the recent years, researchers proposed a number of successful methods to perform out-of-distribution (OOD) detection in deep neural networks (DNNs). So far the scope of the highly accurate methods has been limited to image level classification tasks. However, attempts for generally applicable methods beyond classification did not attain similar performance. In this paper, we address this limitation by proposing a simple yet effective task-agnostic OOD detection method. We estimate the probability density functions (pdfs) of intermediate features of a pre-trained DNN by performing kernel...</td>\n","      <td>['Autonomous Driving', 'Classification', 'Density Estimation', 'General Classification', 'Image Segmentation', 'Medical Diagnosis', 'Medical Image Segmentation', 'Out-of-Distribution Detection', 'Out of Distribution (OOD) Detection', 'Semantic Segmentation']</td>\n","      <td>[Autonomous Driving, Classification, General Classification, Image Segmentation, Medical Diagnosis, Medical Image Segmentation, Semantic Segmentation]</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91a9dc2a-5b53-49c6-859b-4c5819918b71')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-91a9dc2a-5b53-49c6-859b-4c5819918b71 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-91a9dc2a-5b53-49c6-859b-4c5819918b71');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2c394577-fb53-4df9-a87a-bab27b56b9c9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c394577-fb53-4df9-a87a-bab27b56b9c9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2c394577-fb53-4df9-a87a-bab27b56b9c9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["We will be using `valid_df` for all inference testing"],"metadata":{"id":"wVWpZARu0PPf"}},{"cell_type":"markdown","source":["# Fastai & Blurr Inference"],"metadata":{"id":"k48Mzr8yy97P"}},{"cell_type":"code","source":["model_path = \"models/paper-classifier-stage-1-distilroberta.pkl\"\n","learner_inf = load_learner(model_path)"],"metadata":{"id":"SWDpv__nzCYX","executionInfo":{"status":"ok","timestamp":1706449989482,"user_tz":-360,"elapsed":11167,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["learner_inf.blurr_predict(\"random placeholder\")"],"metadata":{"id":"5uoheElKzURd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706449998728,"user_tz":-360,"elapsed":717,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"a63ae88d-eca7-4be8-9223-441ac1401be5"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'labels': [],\n","  'scores': [],\n","  'class_indices': [0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0],\n","  'class_labels': ['Active Learning', 'Image Classification', 'Semantic Segmentation', 'object-detection', 'Object Detection', 'Pose Estimation', 'Information Retrieval', 'Opinion Mining', 'Retrieval', 'valid', 'Self-Supervised Learning', 'Test', 'Benchmarking', 'Segmentation', 'Clustering', 'Community Detection', 'Informativeness', 'Federated Learning', 'Uncertainty Quantification', 'Classification', 'Domain Adaptation', 'regression', 'Text-to-Image Generation', 'Transfer Learning', 'Adversarial Attack', 'Decision Making', 'Variational Inference', '3D Object Detection', 'Object', 'Drug Discovery', 'speech-recognition', 'Speech Recognition', 'counterfactual', 'Reinforcement Learning (RL)', 'Graph Learning', 'Node Classification', 'Unsupervised Domain Adaptation', 'Model Selection', 'Image Segmentation', 'Medical Image Classification', 'Medical Image Segmentation', 'Continual Learning', 'Brain Tumor Segmentation', 'Multi-Label Classification', 'Tumor Segmentation', 'Binary Classification', 'Anatomy', 'Autonomous Vehicles', 'Domain Generalization', 'Anomaly Detection', 'Image Generation', 'Automatic Speech Recognition', 'Automatic Speech Recognition (ASR)', 'Dimensionality Reduction', 'Class Incremental Learning', 'General Classification', 'Incremental Learning', 'reinforcement-learning', 'Meta-Learning', 'Object Recognition', 'Knowledge Distillation', 'Neural Architecture Search', 'Fairness', 'Representation Learning', 'Metric Learning', 'Face Recognition', 'Face Verification', 'Language Modelling', 'Data Augmentation', 'Action Detection', 'Autonomous Driving', 'Contrastive Learning', 'Fact Checking', 'Question Answering', 'World Knowledge', 'Sentiment Analysis', 'Sentiment Classification', 'Attribute', 'Multi-Task Learning', 'Privacy Preserving', 'Quantization', 'Medical Diagnosis', 'Few-Shot Image Classification', 'Few-Shot Learning', 'Image Captioning', 'BIG-bench Machine Learning', 'Knowledge Graphs', 'Image-to-Image Translation', 'Translation', 'Machine Translation', 'Instance Segmentation', 'Memorization', 'Depth Estimation', 'Monocular Depth Estimation', 'Scene Understanding', 'named-entity-recognition', 'Named Entity Recognition', 'Code Generation', 'Instruction Following', 'Disentanglement', 'Object Tracking', 'Visual Question Answering', 'Visual Question Answering (VQA)', 'Novel View Synthesis', 'Adversarial Robustness', 'Denoising', 'Image Retrieval', 'Misinformation', 'Management', 'Lesion Segmentation', 'Zero-Shot Learning', 'Computed Tomography (CT)', 'Feature Engineering', 'Time Series', 'Visual Reasoning', 'EEG', 'Common Sense Reasoning', '3D Reconstruction', 'Graph Embedding', 'Relation', 'Relation Extraction', 'Relation Classification', 'Named Entity Recognition (NER)', 'POS', 'Dependency Parsing', 'Sentence Classification', 'Time Series Analysis', 'Word Embeddings', 'Position', 'Sentence Embedding', 'Sentence-Embedding', 'Inductive Bias', 'Natural Language Inference', 'Semantic Similarity', 'Passage Retrieval', 'Reading Comprehension', 'Masked Language Modeling', 'Prompt Engineering', 'TAG', 'Image Augmentation', 'Vocal Bursts Intensity Prediction', 'Entity Linking', 'Link Prediction', 'Machine Reading Comprehension', 'text-classification', 'Text Classification', 'Action Classification', 'Video Understanding', 'Specificity', 'Graph Attention', 'Large Language Model', 'Document Classification', 'Semantic Textual Similarity', 'Emotion Recognition in Conversation', 'Text Summarization', 'Text Generation', 'Recommendation Systems', 'Logical Reasoning', 'Descriptive', 'Natural Language Understanding', 'NER', 'Image Reconstruction', 'Knowledge Graph Completion', 'Graph Classification', 'Knowledge Graph Embedding', 'Generative Adversarial Network', 'Open-Domain Question Answering', 'Graph Representation Learning', 'Network Embedding', 'Knowledge Graph Embeddings', 'feature selection', 'Optical Character Recognition (OCR)', 'Navigate', 'Ensemble Learning', 'Extractive Summarization', 'Hallucination', 'XLM-R', 'Re-Ranking', 'Multiple-choice', 'Abstractive Text Summarization', 'Document Summarization', 'Answer Selection', 'Chatbot', 'Cross-Lingual Transfer', 'Hate Speech Detection', 'Multi Label Text Classification', 'Multi-Label Text Classification', 'NMT', 'Text Retrieval', 'Zero-Shot Cross-Lingual Transfer', 'Question Generation', 'Question-Generation', 'Data-to-Text Generation', 'Sentence Embeddings', 'Part-Of-Speech Tagging', 'Video Question Answering', 'Word Similarity', 'Open-Ended Question Answering', 'Semantic Parsing', 'Grammatical Error Correction', 'Fake News Detection', 'Style Transfer', 'Pseudo Label', 'Person Re-Identification', 'Action Recognition', 'Temporal Action Localization', 'Video Generation', 'Video Retrieval', 'Deblurring', 'Image Denoising', 'Image Restoration', 'Skeleton Based Action Recognition', 'Super-Resolution', 'Electroencephalogram (EEG)', 'Optical Flow Estimation', 'Panoptic Segmentation', 'Depth Prediction', 'Cell Segmentation', 'Unsupervised Image-To-Image Translation', 'SSIM', 'Image Super-Resolution', 'Emotion Recognition', 'Speech Emotion Recognition', 'STS', 'Community Question Answering', 'Text Categorization', 'Plant Phenotyping', 'Skin Lesion Segmentation', 'Scene Parsing', 'Reflection Removal', 'Foreground Segmentation', 'Aspect-Based Sentiment Analysis', 'Aspect-Based Sentiment Analysis (ABSA)', 'Topic Models', 'Unsupervised Machine Translation', 'Bilingual Lexicon Induction', 'Offline RL', 'Multimodal Machine Translation', 'Talking Face Generation', 'Image Inpainting', 'Conditional Image Generation', 'Face Generation', 'Image Harmonization', 'Multi-Document Summarization', 'Story Generation', 'Spelling Correction', 'Visual Storytelling', 'Learning Word Embeddings', 'Action Localization', 'Text-To-SQL', 'Deep Hashing', 'Conversational Question Answering', 'Knowledge Base Question Answering', 'Transliteration', 'Automatic Post-Editing', 'Emotion Classification', 'Multimodal Emotion Recognition', 'Sentence'],\n","  'probs': [0.00024398656387347728,\n","   0.0029304982163012028,\n","   0.00040377574623562396,\n","   0.00031127571128308773,\n","   0.0006550851394422352,\n","   0.0003387269680388272,\n","   0.0066938623785972595,\n","   0.0004238479887135327,\n","   0.09244595468044281,\n","   0.028430981561541557,\n","   0.0004633644421119243,\n","   0.12160994112491608,\n","   0.010385138913989067,\n","   7.966351404320449e-05,\n","   0.010885988362133503,\n","   5.1941911806352437e-05,\n","   0.0012389525072649121,\n","   2.7452782887849025e-05,\n","   0.000398706499254331,\n","   0.0056176865473389626,\n","   0.0013389112427830696,\n","   0.000674320908728987,\n","   0.00015236188482958823,\n","   0.00012211775174364448,\n","   0.0014311743434518576,\n","   8.836043889459688e-06,\n","   0.0012881640577688813,\n","   0.0003270450688432902,\n","   0.0018726170528680086,\n","   5.13374361617025e-05,\n","   3.5284541809232906e-05,\n","   3.6569028452504426e-05,\n","   0.0012284372933208942,\n","   0.002098607365041971,\n","   1.733326462272089e-05,\n","   2.39163691730937e-05,\n","   0.00017088076856452972,\n","   0.0010855926666408777,\n","   2.9412676667561755e-05,\n","   1.296157279284671e-05,\n","   1.1398013157304376e-05,\n","   0.00017397955525666475,\n","   0.0003045738267246634,\n","   0.0026302949991077185,\n","   0.0001813878188841045,\n","   0.003213270800188184,\n","   9.231841977452859e-05,\n","   0.0007647007005289197,\n","   0.0016811065142974257,\n","   0.00038391660200431943,\n","   0.002362936269491911,\n","   2.7531568775884807e-05,\n","   2.030884388659615e-05,\n","   0.0018490402726456523,\n","   0.0003633831802289933,\n","   0.0295985359698534,\n","   0.0013807793147861958,\n","   0.0011288776295259595,\n","   0.00048098465777002275,\n","   0.0003232395101804286,\n","   0.00014193373499438167,\n","   0.003071219427511096,\n","   0.0005106686148792505,\n","   0.0005006263963878155,\n","   0.0025387504138052464,\n","   0.00020158746337983757,\n","   7.617878873134032e-05,\n","   0.0074722375720739365,\n","   0.0017757144523784518,\n","   1.2759334822476376e-05,\n","   5.082842835690826e-05,\n","   7.696468856011052e-06,\n","   0.00021968719374854118,\n","   0.008148402906954288,\n","   0.00045631700777448714,\n","   0.004022659733891487,\n","   0.0008258140878751874,\n","   0.00010594889317872003,\n","   0.0002474663488101214,\n","   3.373136860318482e-05,\n","   0.019105050712823868,\n","   0.00021474742970895022,\n","   0.0002345762331970036,\n","   0.005931114312261343,\n","   0.0009311789181083441,\n","   0.004643172491341829,\n","   2.5630104573792778e-05,\n","   6.714959454257041e-05,\n","   0.016847828403115273,\n","   0.05165174975991249,\n","   0.001476094126701355,\n","   0.000648786430247128,\n","   0.0006466287304647267,\n","   0.0002593733079265803,\n","   2.5710867703310214e-05,\n","   0.0005370078724808991,\n","   0.0009215565514750779,\n","   0.0006035382393747568,\n","   8.905918366508558e-05,\n","   1.1411119885451626e-05,\n","   2.4291342924698256e-05,\n","   0.00023261229216586798,\n","   0.0008305430528707802,\n","   7.700931746512651e-05,\n","   0.000483962066937238,\n","   0.00034286611480638385,\n","   0.000975532631855458,\n","   0.006036645732820034,\n","   0.0006588057149201632,\n","   3.08626408696e-06,\n","   0.0020213471725583076,\n","   0.00010855466825887561,\n","   0.001444732304662466,\n","   0.00045911900815553963,\n","   0.00024089537328109145,\n","   3.5495842894306406e-05,\n","   0.0003380601410754025,\n","   0.0004519751528277993,\n","   4.353199983597733e-06,\n","   0.00023061860702000558,\n","   0.0001512706803623587,\n","   6.964526255615056e-05,\n","   0.005991889163851738,\n","   0.002518960740417242,\n","   0.0001583991979714483,\n","   0.010982642881572247,\n","   0.0011879813391715288,\n","   0.004737538285553455,\n","   0.015048522502183914,\n","   0.0001685167517280206,\n","   0.00018355916836299002,\n","   0.0015426641330122948,\n","   0.002309449017047882,\n","   0.0008157266420312226,\n","   0.00013070684508420527,\n","   0.00013914535520598292,\n","   0.00023143405269365758,\n","   0.001005300902761519,\n","   0.0014538627583533525,\n","   5.206048081163317e-05,\n","   0.0014379894128069282,\n","   0.0001227809552801773,\n","   4.808543508261209e-06,\n","   0.0001306156482314691,\n","   0.0007781494059599936,\n","   0.0018921069568023086,\n","   2.2664158677798696e-05,\n","   1.7086779280361952e-06,\n","   0.0006890429649502039,\n","   0.000178932910785079,\n","   0.0012244717217981815,\n","   0.0001219422701979056,\n","   0.0012280470691621304,\n","   9.867307198874187e-06,\n","   0.0014073711354285479,\n","   0.02027999423444271,\n","   6.594546721316874e-05,\n","   4.3595751776592806e-05,\n","   0.0037318996619433165,\n","   0.0002057716337731108,\n","   0.004172938410192728,\n","   9.293539187638089e-05,\n","   7.582905823255714e-07,\n","   7.01155950082466e-05,\n","   1.7988895706366748e-06,\n","   0.0016775602707639337,\n","   0.0050552282482385635,\n","   1.5809449905646034e-05,\n","   0.0005679971072822809,\n","   6.226472578418907e-06,\n","   0.0010105815017595887,\n","   0.0015512753743678331,\n","   0.0010200157994404435,\n","   0.00028619763907045126,\n","   0.00012117620644858107,\n","   0.007755107246339321,\n","   0.00623868964612484,\n","   0.04795562103390694,\n","   0.0013938015326857567,\n","   0.0008884993148967624,\n","   9.551760012982413e-05,\n","   0.007258412893861532,\n","   0.0006506546633318067,\n","   0.00040293822530657053,\n","   0.00030041206628084183,\n","   5.50856493646279e-05,\n","   0.0002020232641370967,\n","   0.011977407149970531,\n","   0.0004418327007442713,\n","   0.00043177721090614796,\n","   0.000393526628613472,\n","   0.0003609017876442522,\n","   0.003821362042799592,\n","   0.0014155469834804535,\n","   0.0010487943654879928,\n","   1.2203785445308313e-05,\n","   3.137802195851691e-05,\n","   0.00732401292771101,\n","   0.0004787636862602085,\n","   9.290792513638735e-05,\n","   0.00014572216605301946,\n","   0.00017179631686303765,\n","   0.0011235831771045923,\n","   0.00037200908991508186,\n","   6.9382949732244015e-06,\n","   1.3588603906100616e-05,\n","   1.744119253999088e-05,\n","   3.621400537667796e-05,\n","   3.8247162592597306e-05,\n","   2.641707942530047e-05,\n","   7.531601113441866e-06,\n","   7.808399823261425e-05,\n","   2.1968988221487962e-05,\n","   1.5647710824850947e-05,\n","   5.4193209507502615e-05,\n","   0.0008756237220950425,\n","   0.0014678073348477483,\n","   2.465968282194808e-05,\n","   0.0005097807734273374,\n","   0.00023479911033064127,\n","   7.146056304918602e-06,\n","   7.516948699048953e-06,\n","   1.6968592717603315e-06,\n","   0.0002870699390769005,\n","   0.0011409894796088338,\n","   0.006141956429928541,\n","   0.005559512414038181,\n","   4.2272313294233754e-07,\n","   9.866605250863358e-05,\n","   1.5110490494407713e-05,\n","   0.0007146931602619588,\n","   0.0001495301112299785,\n","   0.000270255928626284,\n","   0.0017642370657995343,\n","   0.001745750429108739,\n","   0.00039477410609833896,\n","   0.00032072424073703587,\n","   0.00022196659119799733,\n","   1.9656255972222425e-05,\n","   0.0018639748450368643,\n","   0.0009488319046795368,\n","   6.901105371071026e-05,\n","   1.3288749869388994e-06,\n","   9.073445107787848e-05,\n","   0.0008992450311779976,\n","   0.00074658083030954,\n","   5.4052819905336946e-05,\n","   8.236659778049216e-05,\n","   1.0339196705899667e-05,\n","   0.00048047213931567967,\n","   0.0003981960180681199,\n","   7.771725358907133e-05,\n","   7.839474710635841e-05,\n","   0.0033639592584222555,\n","   0.0026206430047750473,\n","   0.00011422047100495547,\n","   8.05850049800938e-06,\n","   0.0006139702745713294]}]"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["learner_inf.blurr_predict(\"random placeholder\")[0]['labels']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aMxgtZBCU_S","executionInfo":{"status":"ok","timestamp":1706450016527,"user_tz":-360,"elapsed":633,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"46181d40-2e53-4d1e-b77a-eca8e256a625"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"QYRg4ExrCi4P"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","def metric_measures(test_df, preds):\n","\n","  targets = [np.asarray(target) for target in test_df['task_cat_list'].to_list()]\n","  outputs = [np.asarray(pred) for pred in preds]\n","\n","\n","  accuracy = metrics.accuracy_score(targets, outputs)\n","  f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n","  f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n","\n","  print(f\"F1 Score (Micro) = {f1_score_micro}\")\n","  print(f\"F1 Score (Macro) = {f1_score_macro}\")\n","\n","  return"],"metadata":{"id":"wzQc7GDWCkbX","executionInfo":{"status":"ok","timestamp":1706450032399,"user_tz":-360,"elapsed":698,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["preds = []\n","for idx, row in tqdm(valid_df.iterrows(), total=len(valid_df)):\n","  desc = row['abstract']\n","  labels = learner_inf.blurr_predict(desc)[0]['labels']\n","  pred_tasks = [0] * len(encode_task_types)\n","  for label in labels:\n","    pred_tasks [encode_task_types[label]] = 1\n","  preds.append(pred_tasks)\n","\n","preds[0][:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["fb19217cbdd34f4eb8fd13bc3b751952","9c093c54b6424b1eb0486dfbdc666e84","8139c698110b44ad9cc99f2ec96c51ac","2bd5977709f8402b81787c05c8a0ec3e","2d14476e313d41879aa7f2375fef9cc6","02fc1d9db9034e30bef1363f45fa068b","f40fe8f29f7f435dbf0c4b55fadc6592","c4ee1656dfb546be8c87d37fece92048","d50b488c260642c4b3d116aaa4c893d3","ba5914ec4f394f40bd0151a6daeaba9f","66f5badcc4a04bef9ce6be971bfa2705"]},"id":"c4-5Qmr1C4aA","executionInfo":{"status":"ok","timestamp":1706450894177,"user_tz":-360,"elapsed":858035,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"1607c2a3-2420-4aee-dc81-3a14cbafe422"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2662 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb19217cbdd34f4eb8fd13bc3b751952"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["metric_measures(valid_df, preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LefQDkZtDu7B","executionInfo":{"status":"ok","timestamp":1706450901807,"user_tz":-360,"elapsed":736,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"6991ae1d-aa5b-48ca-b526-3077b7f56e00"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score (Micro) = 0.8168415083640488\n","F1 Score (Macro) = 0.6573110629504366\n"]}]},{"cell_type":"markdown","source":["# Convert to ONNX"],"metadata":{"id":"OGk37-eOI83k"}},{"cell_type":"markdown","source":["### ONNX\n","ONNX (Open Neural Network Exchange) is a open standard for representing machine learning models.\n","- It allows developers to move models between different frameworks (such as PyTorch, TensorFlow, and Caffe2) without losing performance or accuracy.\n","ONNX makes it easier to build and run AI models on a variety of hardware, including GPUs, CPUs, and custom accelerators.\n","- This standard helps eliminate the need for multiple conversion tools and provides a unified representation of the model across different tools and frameworks.\n","- ONNX is being developed by a collaboration of companies including Microsoft, Facebook, Amazon, and IBM, among others, making it a well-supported and widely-adopted standard.\n","- Converting to ONNX runtime often makes model faster."],"metadata":{"id":"StfZ_qBHKKpJ"}},{"cell_type":"code","source":["model_path = \"models/paper-classifier-stage-1-distilroberta.pkl\"\n","learner_inf = load_learner(model_path)"],"metadata":{"id":"m1GEDa3iK6fp","executionInfo":{"status":"ok","timestamp":1706450921415,"user_tz":-360,"elapsed":2129,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["classifier = learner_inf.model.hf_model.eval()\n","\n","torch.onnx.export(\n","    classifier,\n","    torch.LongTensor([[0] * 512]),\n","    'models/paper-classifier.onnx',\n","    input_names=['input_ids'],\n","    output_names=['output'],\n","    opset_version=13,\n","    dynamic_axes={\n","        'input_ids': {0: 'batch_size', 1: 'sequence_len'},\n","        'output': {0: 'batch_size'}\n","    }\n",")"],"metadata":{"id":"L70lkPbHJAMP","executionInfo":{"status":"ok","timestamp":1706450932501,"user_tz":-360,"elapsed":5937,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["pip install onnxruntime\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKRxp71Uu__r","executionInfo":{"status":"ok","timestamp":1706450946236,"user_tz":-360,"elapsed":8847,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"afece6e5-648c-477a-8baf-ff60fe7f8e2d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Using cached onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.16.3\n"]}]},{"cell_type":"code","source":["pip install onnx\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pk_QWDbSvS4q","executionInfo":{"status":"ok","timestamp":1706450976034,"user_tz":-360,"elapsed":16302,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"87cefd63-4872-45eb-bf08-25659ac97ea1"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.15.0\n"]}]},{"cell_type":"code","source":["from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","onnx_model_path = 'models/paper-classifier.onnx'\n","quantized_onnx_model_path = 'models/paper-classifier-quantized.onnx'\n","\n","quantize_dynamic(\n","    onnx_model_path,\n","    quantized_onnx_model_path,\n","    weight_type=QuantType.QUInt8,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlCiUB_6LPmO","executionInfo":{"status":"ok","timestamp":1706451016475,"user_tz":-360,"elapsed":32262,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"ebaa7af2-40e5-44e8-ed5d-1db33c9db1d1"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"]},{"output_type":"stream","name":"stdout","text":["Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.0/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.1/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.2/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.3/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.4/attention/self/MatMul_1]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul]\n","Ignore MatMul due to non constant B: /[/roberta/encoder/layer.5/attention/self/MatMul_1]\n"]}]},{"cell_type":"code","source":["from onnxruntime.quantization import quantize_dynamic, QuantType\n","\n","onnx_model_path = 'models/paper-classifier.onnx'\n","quantized_onnx_model_path = 'models/paper-classifier-quantized.onnx'\n","\n","try:\n","    quantize_dynamic(\n","        onnx_model_path,\n","        quantized_onnx_model_path,\n","        weight_type=QuantType.QUInt8,\n","    )\n","    print(\"Quantization successful.\")\n","except Exception as e:\n","    print(f\"Quantization error: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuBLDfR7wRyj","executionInfo":{"status":"ok","timestamp":1706339421109,"user_tz":-360,"elapsed":372,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"71583e68-8441-4037-ae2a-179e703b9341"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Quantization error: Unable to open proto file: models/paper-classifier.onnx. Please check if it is a valid proto. \n"]}]},{"cell_type":"markdown","source":["# ONNX Inference"],"metadata":{"id":"cXpFGJ4BLjjZ"}},{"cell_type":"markdown","source":["## Normal ONNX"],"metadata":{"id":"dnAvM_OULzYi"}},{"cell_type":"code","source":["import onnxruntime as rt\n","from transformers import AutoTokenizer\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n","\n","class_labels = list(encode_task_types.keys())\n","\n","inf_session = rt.InferenceSession('models/paper-classifier.onnx')\n","input_name = inf_session.get_inputs()[0].name\n","output_name = inf_session.get_outputs()[0].name"],"metadata":{"id":"oJ2DTh1ELmeL","executionInfo":{"status":"ok","timestamp":1706451034707,"user_tz":-360,"elapsed":9837,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"colab":{"base_uri":"https://localhost:8080/","height":267,"referenced_widgets":["6946dbd9b51740e78f39f4ff4433a404","000a6a0d527145f5b0964b0c758aa3a7","640cb6f2bb5845f9a423a310a651a4c7","aa53d63d59ed48a0baad3879e620ee7d","e4cf573fda9748a7a044635165e5eb07","491566a6c56a4e09b8a8e46b754b2b4d","312209e0efd4479d8b39163b4d98f98c","af7ad166edc24570b74ef5b21fd872e8","f4f5ba58fa0540acacc72951609090d1","629d433ef3db45cf9cad264cc45b9445","b84725905ca6402e8172f65a4d253a59","7117d129c64e482ba2a9f7d4ad3c47f0","20d023a56e9841cc8bccdef44c25137a","336e2dc6d1074f30aa60bee1e71f5dde","d0e2c785bfe64b41a0264a40b6a4a235","8e267ce2f0c0495eb2c860b658a55489","eb9ad6e37fbd4ff3b1ce0bfa574a5b91","d63e0678aa7446659e372ffacfac939f","a1384c9243b64d97b1160de2959bedc4","58267cb694e0449c93aef33395d1064e","f3e3767727194bc0ac6323818a7a1ef1","b643eba3824d457abe321be552ffdd97","f3438abe56be46a98990783c2cab570d","c9c32d832548423baacc9f440c07854e","67124991950f4e58a671cd933fe1fe4f","f7bf5ee1e38c48c69f708ac0fd45dfe8","e6e44de7bac642f292d9974e1df6a600","0004d129f70c465898028a42d52f6587","a500df2904f940e4b01b48ec6b2f919b","9128edbd44c94aefa1f134f4c0e4a390","29a5103dc5f64c479be2225e21d842f9","1ce3ff57d70142d0be875e5c4369bd86","6189fbf5150243b881f8c4d7377f8d5c","7bd7909a3ce6411a82cdb661db9accf9","ed5582648e034e37924da2fea3823146","fb5dd050abe7499eaaeec916de66412c","e287f2ecb9d34d1895b0c10136b7882b","2ea4d0f72f734c4b97ff50eb6460cc29","3d6f2abb4c7a4393aba8a50d5336ed0a","b61ea826be784fe6969771dfb15116be","88d564c8c9bf4bb1907941cdbc451ba5","1cd3ded7eafe4b29be0d2f04f8d6e133","cba8b8a18246477494cc37fa166cb029","a869b9c652dd4f729dcf0cfe1c8d7de4"]},"outputId":"ebf2284e-1fb9-495f-a934-f00213938a19"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6946dbd9b51740e78f39f4ff4433a404"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7117d129c64e482ba2a9f7d4ad3c47f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3438abe56be46a98990783c2cab570d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd7909a3ce6411a82cdb661db9accf9"}},"metadata":{}}]},{"cell_type":"code","source":["preds = []\n","for idx, row in tqdm(valid_df.iterrows(), total=valid_df.shape[0]):\n","  desc = row['abstract']\n","  input_ids = tokenizer(desc)['input_ids'][:512]\n","\n","  probs = inf_session.run([output_name], {input_name: [input_ids]})[0]\n","  probs = torch.FloatTensor(probs)\n","\n","  masks = torch.sigmoid(probs) >= 0.5\n","  labels = [class_labels[idx] for idx, mask in enumerate(masks[0]) if mask]\n","\n","  pred_tasks = [0] * len(encode_task_types)\n","  for label in labels:\n","    pred_tasks[encode_task_types[label]] = 1\n","  preds.append(pred_tasks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9d9054a0bcd64b969a737263ed167a92","a127127d3ddf4c8cb069f9d8eb4e2e7b","0452d6fee141448cb475235ec4e52057","5d8ccfe08fd642f9b65e2014bd80b451","a26c810d672a41f480ad7bc4e93be231","7ed5924c54f44e67a74c39f33ca37ff0","e7932a10e9b347998efe09f0c4487180","7a17509b7c5f446888041bb0ee9052c5","41d684d543ba4736a4b21892c2c53f86","9223b10ebd8b42d4b372530ebeaea70c","425beda55be544b9a06b0da219edf75d"]},"id":"CHNlselRMGCZ","executionInfo":{"status":"ok","timestamp":1706451884704,"user_tz":-360,"elapsed":802697,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"54dc1d13-8dea-440a-93eb-f1aad64f91cc"},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2662 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9054a0bcd64b969a737263ed167a92"}},"metadata":{}}]},{"cell_type":"code","source":["metric_measures(valid_df, preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svMa5SB6NGVN","executionInfo":{"status":"ok","timestamp":1706451897336,"user_tz":-360,"elapsed":4937,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"122d45f4-0e67-4bc1-ad3f-40a5127e35d8"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score (Micro) = 0.8166855845629966\n","F1 Score (Macro) = 0.6581541747589666\n"]}]},{"cell_type":"markdown","source":["## Quantized ONNX"],"metadata":{"id":"7X00cpvON8ja"}},{"cell_type":"code","source":["import onnxruntime as rt\n","from transformers import AutoTokenizer\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n","\n","class_labels = list(encode_task_types.keys())\n","\n","inf_session = rt.InferenceSession('models/paper-classifier-quantized.onnx')\n","input_name = inf_session.get_inputs()[0].name\n","output_name = inf_session.get_outputs()[0].name"],"metadata":{"id":"YXQvwlTjN35N","executionInfo":{"status":"ok","timestamp":1706451912094,"user_tz":-360,"elapsed":1754,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["preds = []\n","for idx, row in tqdm(valid_df.iterrows(), total=valid_df.shape[0]):\n","  desc = row['abstract']\n","  input_ids = tokenizer(desc)['input_ids'][:512]\n","\n","  probs = inf_session.run([output_name], {input_name: [input_ids]})[0]\n","  probs = torch.FloatTensor(probs)\n","\n","  masks = torch.sigmoid(probs) >= 0.5\n","  labels = [class_labels[idx] for idx, mask in enumerate(masks[0]) if mask]\n","\n","  pred_tasks = [0] * len(encode_task_types)\n","  for label in labels:\n","    pred_tasks[encode_task_types[label]] = 1\n","  preds.append(pred_tasks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c621e95c81ae4d18823669362e5d09a7","b35445ed12724b04820b1303277fcab1","ac360da750bc422388c4258040d0031e","b03e3f9c2a5c4412bc5380aa81139fec","cd9e343a3a2c451c85139c9aa7e64c34","2fc992abf37d4bd0ad585b4545610034","17d1843bdbe948d29a19f7788d6ac174","8faba08d039045dd9cc851ea53217263","178919022a3142f1b65d9627e5d871a6","8da095032c674e7088db1ec881026f0c","9439974063584dc782a2069c9d7b9548"]},"id":"GnHUnQdFOECs","executionInfo":{"status":"ok","timestamp":1706452665347,"user_tz":-360,"elapsed":201016,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"9329a41f-743a-48f4-9046-48ad85b12c52"},"execution_count":40,"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c621e95c81ae4d18823669362e5d09a7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2662 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":["metric_measures(valid_df, preds) #"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzqtaeWbOFmJ","executionInfo":{"status":"ok","timestamp":1706452739992,"user_tz":-360,"elapsed":619,"user":{"displayName":"Tabassum Tanzim Nawar","userId":"02679135628341457805"}},"outputId":"52a4629d-712a-4409-f27b-6954ea4eefff"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score (Micro) = 0.8106302643043857\n","F1 Score (Macro) = 0.6385088352431093\n"]}]}]}